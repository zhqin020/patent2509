#!/usr/bin/env python3
"""
è½¦è¾†å·¦è½¬è½¨è¿¹é¢„æµ‹ç³»ç»Ÿ - æ¸…ç†ç‰ˆæœ¬
åŸºäºå†å²è½¨è¿¹çš„çœŸæ­£å·¦è½¬æ„å›¾é¢„æµ‹æ¡†æ¶
åŒ…å«æµ‹è¯•å‰åçš„è½¨è¿¹å¯è§†åŒ–åŠŸèƒ½
"""

import sys
import os
import time
from typing import Dict, List, Tuple, Optional
import yaml
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
import cv2
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm

# å¯¼å…¥å·¦è½¬æ•°æ®åˆ†æè„šæœ¬
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
from å·¦è½¬æ•°æ®åˆ†æè„šæœ¬ import LeftTurnAnalyzer


# =============================
# é…ç½®åŠ è½½å™¨
# =============================
def load_config(config_path=None):
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    if config_path is None:
        script_dir = os.path.dirname(os.path.abspath(__file__))
        config_path = os.path.join(script_dir, "config.yaml")
    
    with open(config_path, "r", encoding="utf-8") as f:
        config = yaml.safe_load(f)
    return config


# =============================
# æ•°æ®ç®¡é“ç±»
# =============================
class DataPipeline:
    """æ•°æ®å¤„ç†ç®¡é“ç±»ï¼Œç”¨äºç»Ÿä¸€ç®¡ç†ä»åŸå§‹æ•°æ®åˆ°æ•°æ®é›†çš„è½¬æ¢è¿‡ç¨‹"""
    def __init__(self, raw_path):
        self.raw_path = raw_path
    
    def build_dataset(self, int_id=None, approach=None, history_length=30, prediction_horizon=12, 
                     min_trajectory_length=42, max_samples=None):  # è¿›ä¸€æ­¥é™ä½æœ€å°è½¨è¿¹é•¿åº¦è¦æ±‚
        """ä»åŸå§‹æ•°æ®æ„å»ºæ•°æ®é›†"""
        print("ğŸ”„ å¼€å§‹æ„å»ºæ•°æ®é›†...")
        
        # åŠ è½½åŸå§‹æ•°æ®
        raw_data = pd.read_csv(self.raw_path)
        
        # æ ¹æ®ç”¨æˆ·é€‰æ‹©çš„è·¯å£å’Œæ–¹å‘è¿›è¡Œç­›é€‰
        if int_id is not None:
            filtered = raw_data[raw_data['int_id'] == int_id]
            print(f"âœ… å·²è¿‡æ»¤è·¯å£ {int_id} ç›¸å…³è½¦è¾†æ•°æ®: {len(filtered)}/{len(raw_data)} æ¡è®°å½•")
            
            if approach is not None:
                filtered = filtered[filtered['direction'] == approach]
                print(f"âœ… å·²è¿‡æ»¤å…¥å£æ–¹å‘ {approach}: {len(filtered)}/{len(raw_data)} æ¡è®°å½•")
        elif approach is not None:
            filtered = raw_data[raw_data['direction'] == approach]
            print(f"âœ… å·²è¿‡æ»¤å…¥å£æ–¹å‘ {approach}: {len(filtered)}/{len(raw_data)} æ¡è®°å½•")
        else:
            filtered = raw_data
            print("âœ… ä½¿ç”¨å…¨éƒ¨æ•°æ®ï¼Œä¸è¿›è¡Œè·¯å£å’Œæ–¹å‘ç­›é€‰")
        
        # æ˜¾ç¤ºæ•°æ®ç»Ÿè®¡ä¿¡æ¯
        total_vehicles = len(filtered['vehicle_id'].unique())
        left_turn_vehicles = 0
        if 'movement' in filtered.columns:
            left_turn_vehicles = len(filtered[filtered['movement'] == 2]['vehicle_id'].unique())
            left_turn_percentage = (left_turn_vehicles / total_vehicles * 100) if total_vehicles > 0 else 0
            print(f"âœ… æ•°æ®å‡†å¤‡å®Œæˆ: {len(filtered)} æ¡è®°å½•, {total_vehicles} è¾†è½¦")
            print(f"   å…¶ä¸­å·¦è½¬è½¦è¾†: {left_turn_vehicles} è¾† ({left_turn_percentage:.1f}%)")
        
        # åˆ›å»ºæ•°æ®é›†
        dataset = MultiModalDataset(
            data_path=self.raw_path,
            history_length=history_length,
            prediction_horizon=prediction_horizon,
            min_trajectory_length=min_trajectory_length,
            max_samples=max_samples,
            filtered_data=filtered
        )
        
        return dataset
    
    def get_dataset_statistics(self, dataset):
        """è·å–æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯"""
        if hasattr(dataset, 'analyze_dataset'):
            dataset.analyze_dataset()
    
    def split_dataset(self, dataset, train_ratio=0.7, val_ratio=0.15):
        """å°†æ•°æ®é›†åˆ’åˆ†ä¸ºè®­ç»ƒé›†ã€éªŒè¯é›†å’Œæµ‹è¯•é›†"""
        dataset_size = len(dataset)
        
        # æ£€æŸ¥æ•°æ®é›†å¤§å° - ä½†ä¸ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®æ›¿æ¢
        if dataset_size == 0:
            print("âŒ æ•°æ®é›†ä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œè®­ç»ƒ")
            raise ValueError("æ•°æ®é›†ä¸ºç©ºï¼Œè¯·æ£€æŸ¥æ•°æ®æºå’Œç­›é€‰æ¡ä»¶")
        
        if dataset_size < 3:
            print(f"âš ï¸ æ•°æ®é›†å¤ªå°({dataset_size}ä¸ªæ ·æœ¬)ï¼Œä½†å°†ç»§ç»­ä½¿ç”¨çœŸå®æ•°æ®")
            # å¯¹äºæå°æ•°æ®é›†ï¼Œç®€å•åˆ’åˆ†
            if dataset_size == 1:
                train_size, val_size, test_size = 1, 0, 0
            elif dataset_size == 2:
                train_size, val_size, test_size = 1, 1, 0
            else:  # dataset_size == 3
                train_size, val_size, test_size = 1, 1, 1
        else:
            train_size = max(1, int(train_ratio * dataset_size))
            val_size = max(1, int(val_ratio * dataset_size))
            test_size = max(1, dataset_size - train_size - val_size)
            
            # ç¡®ä¿æ€»æ•°ä¸è¶…è¿‡æ•°æ®é›†å¤§å°
            if train_size + val_size + test_size > dataset_size:
                train_size = max(1, dataset_size - 2)
                val_size = 1
                test_size = 1
        
        # éšæœºåˆ’åˆ†
        indices = torch.randperm(dataset_size).tolist()
        train_indices = indices[:train_size]
        val_indices = indices[train_size:train_size + val_size] if val_size > 0 else []
        test_indices = indices[train_size + val_size:train_size + val_size + test_size] if test_size > 0 else []
        
        train_dataset = torch.utils.data.Subset(dataset, train_indices)
        val_dataset = torch.utils.data.Subset(dataset, val_indices) if val_indices else train_dataset
        test_dataset = torch.utils.data.Subset(dataset, test_indices) if test_indices else train_dataset
        
        print(f"âœ… æ•°æ®é›†åˆ’åˆ†å®Œæˆ:")
        print(f"   è®­ç»ƒé›†: {len(train_dataset)} æ ·æœ¬")
        print(f"   éªŒè¯é›†: {len(val_dataset)} æ ·æœ¬")
        print(f"   æµ‹è¯•é›†: {len(test_dataset)} æ ·æœ¬")
        
        return train_dataset, val_dataset, test_dataset
    

    
    def analyze_dataset_split(self, train_dataset, val_dataset, test_dataset):
        """åˆ†æåˆ’åˆ†åå„æ•°æ®é›†çš„å·¦è½¬è½¦è¾†åˆ†å¸ƒ"""
        print("\nğŸ“Š å„æ•°æ®é›†å·¦è½¬è½¦è¾†åˆ†å¸ƒç»Ÿè®¡:")
        
        datasets = [
            ("è®­ç»ƒé›†", train_dataset),
            ("éªŒè¯é›†", val_dataset),
            ("æµ‹è¯•é›†", test_dataset)
        ]
        
        for name, dataset in datasets:
            left_count = 0
            total_count = len(dataset)
            
            for i in range(total_count):
                sample = dataset[i]
                if sample['left_turn_intent'].item() > 0.5:
                    left_count += 1
            
            left_ratio = (left_count / total_count * 100) if total_count > 0 else 0
            print(f"   {name}: {left_count}/{total_count} ({left_ratio:.1f}%) å·¦è½¬æ ·æœ¬")


# =============================
# æ¨¡æ‹Ÿæ•°æ®é›†ç±»
# =============================
class MockDataset(Dataset):
    """æ¨¡æ‹Ÿæ•°æ®é›†ç±»ï¼Œç”¨äºæ¼”ç¤ºå’Œæµ‹è¯•"""
    def __init__(self, size=1000, history_length=3):
        self.size = size
        self.history_length = history_length
        self.samples = []
        
        # ç”Ÿæˆæ¨¡æ‹Ÿæ ·æœ¬
        for i in range(size):
            # æ¨¡æ‹Ÿå·¦è½¬æ ·æœ¬ï¼ˆ20%æ¦‚ç‡ï¼‰
            is_left_turn = np.random.random() < 0.2
            
            # ç”Ÿæˆå†å²è½¨è¿¹
            history_traj = np.random.randn(history_length, 2).cumsum(axis=0)
            
            # ç”Ÿæˆæœªæ¥è½¨è¿¹
            if is_left_turn:
                # å·¦è½¬è½¨è¿¹
                future_traj = np.array([[i, i*0.5] for i in range(12)])
            else:
                # ç›´è¡Œè½¨è¿¹
                future_traj = np.array([[i, 0] for i in range(12)])
            
            sample = {
                'label': 1 if is_left_turn else 0,
                'left_turn_intent': 1.0 if is_left_turn else 0.0,  # æ·»åŠ left_turn_intentå±æ€§
                'history_trajectory': history_traj,
                'future_trajectory': future_traj,
                'vehicle_id': f'mock_vehicle_{i}',
                'info': {
                    'history_trajectory': history_traj,
                    'future_trajectory': future_traj,
                    'vehicle_id': f'mock_vehicle_{i}'
                }
            }
            self.samples.append(sample)
    
    def __len__(self):
        return self.size
    
    def __getitem__(self, idx):
        sample = self.samples[idx]
        
        return {
            'visual_features': torch.randn(3, 64, 64),  # ä¿®å¤ä¸ºNCHWæ ¼å¼
            'motion_features': torch.tensor(sample['info']['history_trajectory'], dtype=torch.float32),
            'traffic_features': torch.randn(self.history_length, 10),
            'left_turn_intent': torch.tensor(float(sample['label']), dtype=torch.float32),
            'target_trajectory': torch.tensor(sample['info']['future_trajectory'], dtype=torch.float32)
        }


# =============================
# å¤šæ¨¡æ€æ•°æ®é›†ç±»
# =============================
class MultiModalDataset(Dataset):
    """å¤šæ¨¡æ€æ•°æ®é›†ç±» - æ”¯æŒçœŸæ­£çš„å·¦è½¬é¢„æµ‹ä»»åŠ¡"""
    
    def __init__(self, data_path: str, history_length: int = 3, prediction_horizon: int = 2,
                 min_trajectory_length: int = 5, max_samples: Optional[int] = None, filtered_data=None):
        """åˆå§‹åŒ–æ•°æ®é›†"""
        self.data_path = data_path
        self.history_length = history_length
        self.prediction_horizon = prediction_horizon
        self.min_trajectory_length = min_trajectory_length
        self.max_samples = max_samples
        self.filtered_data = filtered_data
        
        self.samples = []
        self.load_data()
    
    def load_data(self):
        """åŠ è½½é¢„å¤„ç†å¥½çš„å·¦è½¬æ•°æ®"""
        print(f"æ­£åœ¨åŠ è½½é¢„å¤„ç†æ•°æ®: {self.data_path}")
        
        if self.filtered_data is not None:
            self.data = self.filtered_data
        else:
            self.data = pd.read_csv(self.data_path)
        
        print(f"âœ… æ•°æ®åŠ è½½å®Œæˆ: {len(self.data)} æ¡è®°å½•")
        self._build_prediction_samples()
    
    def _build_prediction_samples(self):
        """æ„å»ºçœŸæ­£çš„é¢„æµ‹æ ·æœ¬"""
        samples = []
        
        # æ£€æŸ¥æ•°æ®åˆ—åå¹¶è¿›è¡Œæ˜ å°„
        column_mapping = self._map_columns()
        
        # æŒ‰è½¦è¾†IDåˆ†ç»„
        vehicle_id_col = column_mapping.get('vehicle_id', 'vehicle_id')
        frame_id_col = column_mapping.get('frame_id', 'frame_id')
        x_col = column_mapping.get('x', 'x')
        y_col = column_mapping.get('y', 'y')
        
        vehicle_groups = self.data.groupby(vehicle_id_col)
        
        print(f"ğŸ” å¼€å§‹å¤„ç† {len(vehicle_groups)} è¾†è½¦çš„è½¨è¿¹æ•°æ®...")
        print(f"ğŸ” æœ€å°è½¨è¿¹é•¿åº¦è¦æ±‚: {self.min_trajectory_length} å¸§")
        print(f"ğŸ” å†å²é•¿åº¦: {self.history_length}, é¢„æµ‹é•¿åº¦: {self.prediction_horizon}")
        
        processed_vehicles = 0
        skipped_short = 0
        skipped_insufficient = 0
        
        for vehicle_id, vehicle_data in vehicle_groups:
            processed_vehicles += 1
            if processed_vehicles <= 5:  # åªæ‰“å°å‰5è¾†è½¦çš„è¯¦ç»†ä¿¡æ¯
                print(f"ğŸš— å¤„ç†è½¦è¾† {vehicle_id}: {len(vehicle_data)} å¸§æ•°æ®")
            
            if frame_id_col in vehicle_data.columns:
                vehicle_data = vehicle_data.sort_values(frame_id_col)
            else:
                # å¦‚æœæ²¡æœ‰frame_idï¼ŒæŒ‰ç´¢å¼•æ’åº
                vehicle_data = vehicle_data.reset_index(drop=True)
            
            # æ£€æŸ¥è½¨è¿¹é•¿åº¦ - å¤§å¹…é™ä½è¦æ±‚
            if len(vehicle_data) < self.min_trajectory_length:
                skipped_short += 1
                if processed_vehicles <= 5:
                    print(f"   âš ï¸ è·³è¿‡: è½¨è¿¹å¤ªçŸ­ ({len(vehicle_data)} < {self.min_trajectory_length})")
                continue
            
            # ä¸ºæ¯ä¸ªè½¦è¾†åˆ›å»ºå¤šä¸ªé¢„æµ‹æ ·æœ¬ - å¤§å¹…å¢åŠ é‡‡æ ·å¯†åº¦
            available_length = len(vehicle_data) - self.history_length - self.prediction_horizon
            if available_length <= 0:
                skipped_insufficient += 1
                if processed_vehicles <= 5:
                    print(f"   âš ï¸ è·³è¿‡: å¯ç”¨é•¿åº¦ä¸è¶³ ({available_length} <= 0)")
                continue
                
            # æ›´å¯†é›†çš„é‡‡æ ·ï¼šæ¯3å¸§é‡‡æ ·ä¸€æ¬¡ï¼Œç¡®ä¿å……åˆ†åˆ©ç”¨æ•°æ®
            step_size = max(1, min(3, available_length // 5))
            samples_from_vehicle = 0
            
            for i in range(self.history_length, len(vehicle_data) - self.prediction_horizon, step_size):
                samples_from_vehicle += 1
                history_data = vehicle_data.iloc[i-self.history_length:i]
                future_data = vehicle_data.iloc[i:i+self.prediction_horizon]
                
                # æå–è½¨è¿¹
                try:
                    history_trajectory = history_data[[x_col, y_col]].values
                    future_trajectory = future_data[[x_col, y_col]].values
                except KeyError as e:
                    print(f"âš ï¸ åˆ—åé”™è¯¯: {e}, å¯ç”¨åˆ—: {list(vehicle_data.columns)}")
                    continue
                
                # è·å–å·¦è½¬æ„å›¾æ ‡ç­¾
                left_turn_intent = self.get_left_turn_intent(future_data, column_mapping)
                
                sample = {
                    'vehicle_id': vehicle_id,
                    'history_trajectory': history_trajectory,
                    'future_trajectory': future_trajectory,
                    'left_turn_intent': left_turn_intent,
                    'info': {
                        'vehicle_id': vehicle_id,
                        'history_trajectory': history_trajectory,
                        'future_trajectory': future_trajectory
                    }
                }
                
                samples.append(sample)
                
                # é™åˆ¶æ ·æœ¬æ•°é‡
                if self.max_samples and len(samples) >= self.max_samples:
                    break
            
            if processed_vehicles <= 5:
                print(f"   âœ… ä»è½¦è¾† {vehicle_id} æå–äº† {samples_from_vehicle} ä¸ªæ ·æœ¬")
            
            if self.max_samples and len(samples) >= self.max_samples:
                break
        
        self.samples = samples
        print(f"ğŸ“Š æ•°æ®é›†æ„å»ºç»Ÿè®¡:")
        print(f"   å¤„ç†è½¦è¾†æ€»æ•°: {processed_vehicles}")
        print(f"   è·³è¿‡(è½¨è¿¹å¤ªçŸ­): {skipped_short}")
        print(f"   è·³è¿‡(å¯ç”¨é•¿åº¦ä¸è¶³): {skipped_insufficient}")
        print(f"   æˆåŠŸæ„å»ºæ ·æœ¬: {len(self.samples)}")
        print(f"âœ… æ„å»ºå®Œæˆ: {len(self.samples)} ä¸ªé¢„æµ‹æ ·æœ¬")
    
    def _map_columns(self):
        """æ˜ å°„æ•°æ®åˆ—å"""
        column_mapping = {}
        columns = self.data.columns.tolist()
        
        # æ‰“å°å¯ç”¨åˆ—åç”¨äºè°ƒè¯•
        print(f"ğŸ” æ•°æ®åˆ—å: {columns}")
        
        # æ˜ å°„å¸¸è§çš„åˆ—åå˜ä½“
        for col in columns:
            col_lower = col.lower()
            if col_lower in ['x', 'local_x', 'x_pos', 'x_position']:
                column_mapping['x'] = col
            elif col_lower in ['y', 'local_y', 'y_pos', 'y_position']:
                column_mapping['y'] = col
            elif col_lower in ['vehicle_id', 'veh_id', 'id', 'car_id']:
                column_mapping['vehicle_id'] = col
            elif col_lower in ['frame_id', 'frame', 'time', 'timestamp']:
                column_mapping['frame_id'] = col
            elif col_lower in ['movement', 'maneuver', 'turn_type']:
                column_mapping['movement'] = col
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ ‡å‡†æ˜ å°„ï¼Œä½¿ç”¨å‰å‡ åˆ—
        if 'x' not in column_mapping and len(columns) >= 1:
            column_mapping['x'] = columns[0]
        if 'y' not in column_mapping and len(columns) >= 2:
            column_mapping['y'] = columns[1]
        if 'vehicle_id' not in column_mapping and len(columns) >= 3:
            column_mapping['vehicle_id'] = columns[2]
        if 'frame_id' not in column_mapping and len(columns) >= 4:
            column_mapping['frame_id'] = columns[3]
        
        print(f"ğŸ” åˆ—åæ˜ å°„: {column_mapping}")
        return column_mapping
    
    def get_left_turn_intent(self, vehicle_data, column_mapping):
        """ä»æ•°æ®ä¸­è·å–å·¦è½¬æ„å›¾æ ‡ç­¾"""
        movement_col = column_mapping.get('movement')
        
        if movement_col and movement_col in vehicle_data.columns:
            movements = vehicle_data[movement_col].values
            # å¦‚æœæœªæ¥è½¨è¿¹ä¸­åŒ…å«å·¦è½¬(movement=2)ï¼Œåˆ™æ ‡è®°ä¸ºå·¦è½¬æ„å›¾
            return 1.0 if 2 in movements else 0.0
        else:
            # åŸºäºè½¨è¿¹å‡ ä½•ç‰¹å¾åˆ¤æ–­å·¦è½¬æ„å›¾
            try:
                x_col = column_mapping.get('x', 'x')
                y_col = column_mapping.get('y', 'y')
                trajectory = vehicle_data[[x_col, y_col]].values
                return self._detect_left_turn_from_trajectory(trajectory)
            except:
                # å…œåº•ï¼šéšæœºåˆ†é…ï¼ˆæ¨¡æ‹ŸçœŸå®åˆ†å¸ƒï¼‰
                return 1.0 if np.random.random() < 0.2 else 0.0
    
    def _detect_left_turn_from_trajectory(self, trajectory):
        """åŸºäºè½¨è¿¹å‡ ä½•ç‰¹å¾æ£€æµ‹å·¦è½¬æ„å›¾"""
        if len(trajectory) < 5:
            return 0.0
        
        try:
            # è®¡ç®—æ–¹å‘å˜åŒ–
            directions = []
            for i in range(1, len(trajectory)):
                dx = trajectory[i][0] - trajectory[i-1][0]
                dy = trajectory[i][1] - trajectory[i-1][1]
                if dx != 0 or dy != 0:
                    angle = np.arctan2(dy, dx)
                    directions.append(angle)
            
            if len(directions) < 3:
                return 0.0
            
            # è®¡ç®—æ€»çš„è§’åº¦å˜åŒ–
            total_angle_change = 0
            for i in range(1, len(directions)):
                angle_diff = directions[i] - directions[i-1]
                # å¤„ç†è§’åº¦è·³è·ƒ
                if angle_diff > np.pi:
                    angle_diff -= 2 * np.pi
                elif angle_diff < -np.pi:
                    angle_diff += 2 * np.pi
                total_angle_change += angle_diff
            
            # å·¦è½¬é€šå¸¸æœ‰è´Ÿçš„è§’åº¦å˜åŒ–ï¼ˆé¡ºæ—¶é’ˆï¼‰
            left_turn_score = max(0, -total_angle_change / np.pi)
            return min(1.0, left_turn_score)
            
        except:
            return 0.0
    
    def __len__(self):
        """è¿”å›æ•°æ®é›†æ ·æœ¬æ•°é‡"""
        return len(self.samples)
    
    def __getitem__(self, idx):
        """è·å–å•ä¸ªé¢„æµ‹æ ·æœ¬"""
        return self._get_prediction_sample(idx)
    
    def _get_prediction_sample(self, idx):
        """è·å–é¢„æµ‹æ ·æœ¬"""
        if idx >= len(self.samples):
            raise IndexError(f"Index {idx} out of range for dataset of size {len(self.samples)}")
        
        sample = self.samples[idx]
        
        # æå–ç‰¹å¾
        visual_features = self.extract_visual_features(sample['history_trajectory'])
        motion_features = self.extract_motion_features(sample['history_trajectory'])
        traffic_features = self.extract_traffic_features(sample['history_trajectory'])
        
        return {
            'visual_features': torch.tensor(visual_features, dtype=torch.float32),
            'motion_features': torch.tensor(motion_features, dtype=torch.float32),
            'traffic_features': torch.tensor(traffic_features, dtype=torch.float32),
            'left_turn_intent': torch.tensor(sample['left_turn_intent'], dtype=torch.float32),
            'target_trajectory': torch.tensor(sample['future_trajectory'], dtype=torch.float32)
        }
    
    def extract_visual_features(self, history):
        """æå–è§†è§‰ç‰¹å¾"""
        # ç®€åŒ–çš„è§†è§‰ç‰¹å¾æå–ï¼Œè¿”å›NCHWæ ¼å¼
        return np.random.randn(3, 64, 64)
    
    def extract_motion_features(self, history):
        """æå–è¿åŠ¨å­¦ç‰¹å¾"""
        # åŸºç¡€è¿åŠ¨å­¦ç‰¹å¾ï¼šä½ç½®ã€é€Ÿåº¦ã€åŠ é€Ÿåº¦
        positions = history
        velocities = np.diff(positions, axis=0, prepend=positions[0:1])
        accelerations = np.diff(velocities, axis=0, prepend=velocities[0:1])
        
        # ç»„åˆç‰¹å¾
        features = np.concatenate([positions, velocities, accelerations], axis=1)
        return features
    
    def extract_traffic_features(self, history):
        """æå–äº¤é€šç¯å¢ƒç‰¹å¾"""
        # ç®€åŒ–çš„äº¤é€šç‰¹å¾
        return np.random.randn(len(history), 10)


# =============================
# æ¨¡å‹æ¶æ„
# =============================
class VisualEncoder(nn.Module):
    """è§†è§‰ç‰¹å¾ç¼–ç å™¨"""
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)
        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)
        self.pool = nn.AdaptiveAvgPool2d((8, 8))
        self.fc = nn.Linear(64 * 8 * 8, 256)
    
    def forward(self, x):
        # ç¡®ä¿è¾“å…¥æ ¼å¼ä¸º NCHW: [batch, channels, height, width]
        if x.dim() == 4 and x.shape[-1] == 3:  # å¦‚æœæ˜¯ NHWC æ ¼å¼
            x = x.permute(0, 3, 1, 2)  # è½¬æ¢ä¸º NCHW
        
        x = F.relu(self.conv1(x))
        x = F.relu(self.conv2(x))
        x = self.pool(x)
        x = x.view(x.size(0), -1)
        x = self.fc(x)
        return x


class MotionEncoder(nn.Module):
    """è¿åŠ¨ç‰¹å¾ç¼–ç å™¨"""
    def __init__(self, input_dim=6, hidden_dim=128):
        super().__init__()
        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)
        self.fc = nn.Linear(hidden_dim, 256)
    
    def forward(self, x):
        x, _ = self.lstm(x)
        x = x[:, -1, :]  # å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥
        x = self.fc(x)
        return x


class TrafficEncoder(nn.Module):
    """äº¤é€šç¯å¢ƒç‰¹å¾ç¼–ç å™¨"""
    def __init__(self, input_dim=10, hidden_dim=64):
        super().__init__()
        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)
        self.fc = nn.Linear(hidden_dim, 256)  # ä¿®æ”¹ä¸º256ç»´ï¼Œä¸å…¶ä»–ç¼–ç å™¨ä¿æŒä¸€è‡´
    
    def forward(self, x):
        x, _ = self.lstm(x)
        x = x[:, -1, :]
        x = self.fc(x)
        return x


class AttentionFusion(nn.Module):
    """æ³¨æ„åŠ›èåˆæ¨¡å—"""
    def __init__(self, feature_dim=256):
        super().__init__()
        self.attention = nn.MultiheadAttention(feature_dim, num_heads=8)
        self.norm = nn.LayerNorm(feature_dim)
    
    def forward(self, visual, motion, traffic):
        # å †å ç‰¹å¾
        features = torch.stack([visual, motion, traffic], dim=1)  # [batch, 3, feature_dim]
        
        # è‡ªæ³¨æ„åŠ›
        attended, _ = self.attention(features, features, features)
        attended = self.norm(attended + features)
        
        # å¹³å‡æ± åŒ–
        fused = attended.mean(dim=1)
        return fused


class IntentClassifier(nn.Module):
    """å·¦è½¬æ„å›¾åˆ†ç±»å™¨"""
    def __init__(self, input_dim=256):
        super().__init__()
        self.fc1 = nn.Linear(input_dim, 128)
        self.fc2 = nn.Linear(128, 64)
        self.fc3 = nn.Linear(64, 1)
        self.dropout = nn.Dropout(0.3)
    
    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = self.dropout(x)
        x = F.relu(self.fc2(x))
        x = self.dropout(x)
        x = torch.sigmoid(self.fc3(x))
        return x


class TrajectoryDecoder(nn.Module):
    """è½¨è¿¹é¢„æµ‹è§£ç å™¨"""
    def __init__(self, input_dim=256, prediction_horizon=2):
        super().__init__()
        self.prediction_horizon = prediction_horizon
        self.fc1 = nn.Linear(input_dim, 128)
        self.fc2 = nn.Linear(128, 64)
        self.fc3 = nn.Linear(64, prediction_horizon * 2)  # x, y coordinates
    
    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        x = x.view(-1, self.prediction_horizon, 2)
        return x


class LeftTurnPredictor(nn.Module):
    """å·¦è½¬é¢„æµ‹æ¨¡å‹ï¼Œé‡‡ç”¨å¤šæ¨¡æ€èåˆæ¶æ„"""
    def __init__(self, prediction_horizon=2):
        super().__init__()
        self.visual_encoder = VisualEncoder()
        self.motion_encoder = MotionEncoder()
        self.traffic_encoder = TrafficEncoder()
        self.attention_fusion = AttentionFusion()
        self.intent_classifier = IntentClassifier()
        self.trajectory_decoder = TrajectoryDecoder(prediction_horizon=prediction_horizon)
    
    def forward(self, visual_feat, motion_feat, traffic_feat):
        # ç¼–ç å„æ¨¡æ€ç‰¹å¾
        visual_encoded = self.visual_encoder(visual_feat)
        motion_encoded = self.motion_encoder(motion_feat)
        traffic_encoded = self.traffic_encoder(traffic_feat)
        
        # æ³¨æ„åŠ›èåˆ
        fused_features = self.attention_fusion(visual_encoded, motion_encoded, traffic_encoded)
        
        # é¢„æµ‹å·¦è½¬æ„å›¾å’Œè½¨è¿¹
        intent_pred = self.intent_classifier(fused_features)
        trajectory_pred = self.trajectory_decoder(fused_features)
        
        return intent_pred, trajectory_pred


# =============================
# Focal Loss
# =============================
class FocalLoss(nn.Module):
    """Focal Loss å®ç°ï¼Œç”¨äºå¤„ç†ç±»åˆ«ä¸å¹³è¡¡"""
    def __init__(self, alpha=2.0, gamma=2.0, reduction='mean'):
        super().__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.reduction = reduction
    
    def forward(self, inputs, targets):
        ce_loss = F.binary_cross_entropy(inputs, targets, reduction='none')
        pt = torch.exp(-ce_loss)
        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss
        
        if self.reduction == 'mean':
            return focal_loss.mean()
        elif self.reduction == 'sum':
            return focal_loss.sum()
        else:
            return focal_loss


# =============================
# è®­ç»ƒç®¡ç†å™¨
# =============================
class TrainingManager:
    """è®­ç»ƒç®¡ç†å™¨"""
    def __init__(self, model, train_loader, val_loader, device='cuda'):
        self.model = model.to(device)
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.device = device
        
        # è®¡ç®—ç±»åˆ«æƒé‡ - ç»Ÿè®¡è®­ç»ƒé›†ä¸­çš„æ­£è´Ÿæ ·æœ¬æ¯”ä¾‹
        num_pos = 0
        num_neg = 0
        for batch in train_loader:
            labels = batch['left_turn_intent']
            num_pos += (labels > 0.5).sum().item()
            num_neg += (labels <= 0.5).sum().item()
        
        pos_weight = num_neg / max(num_pos, 1)  # é¿å…é™¤é›¶
        print(f"ğŸ“Š è®¡ç®—ç±»åˆ«æƒé‡: å·¦è½¬æ ·æœ¬æ•°={num_pos}, éå·¦è½¬æ ·æœ¬æ•°={num_neg}, å·¦è½¬æ ·æœ¬æƒé‡={pos_weight:.2f}")
        
        # è°ƒæ•´Focal Losså‚æ•° - æé«˜æ­£ç±»æƒé‡ï¼Œé™ä½gammaé¿å…è¿‡åº¦æƒ©ç½š
        self.intent_criterion = FocalLoss(alpha=5.0, gamma=1.0)
        self.trajectory_criterion = nn.MSELoss()
        
        self.optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer, mode='min', patience=5, factor=0.5
        )
        
        self.train_losses = []
        self.val_losses = []
        self.train_intent_losses = []
        self.val_intent_losses = []
        self.train_traj_losses = []
        self.val_traj_losses = []
    
    def train_epoch(self, epoch_num=None, total_epochs=None):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0
        total_intent_loss = 0
        total_traj_loss = 0
        
        with tqdm(total=len(self.train_loader), desc=f"è®­ç»ƒ Epoch {epoch_num}/{total_epochs}", unit="æ‰¹") as pbar:
            for batch in self.train_loader:
                visual_feat = batch['visual_features'].to(self.device)
                motion_feat = batch['motion_features'].to(self.device)
                traffic_feat = batch['traffic_features'].to(self.device)
                intent_target = batch['left_turn_intent'].to(self.device)
                traj_target = batch['target_trajectory'].to(self.device)
                
                self.optimizer.zero_grad()
                
                intent_pred, traj_pred = self.model(visual_feat, motion_feat, traffic_feat)
                
                # è®¡ç®—æŸå¤±
                intent_loss = self.intent_criterion(intent_pred.squeeze(), intent_target)
                traj_loss = self.trajectory_criterion(traj_pred, traj_target)
                
                # åŠ æƒæ€»æŸå¤± - å¤§å¹…æé«˜æ„å›¾è¯†åˆ«çš„æƒé‡ï¼Œé™ä½è½¨è¿¹é¢„æµ‹æƒé‡
                total_batch_loss = 20.0 * intent_loss + 0.01 * traj_loss
                
                total_batch_loss.backward()
                self.optimizer.step()
                
                total_loss += total_batch_loss.item()
                total_intent_loss += intent_loss.item()
                total_traj_loss += traj_loss.item()
                
                pbar.update(1)
                pbar.set_postfix({
                    'æ€»æŸå¤±': f'{total_batch_loss.item():.4f}',
                    'æ„å›¾æŸå¤±': f'{intent_loss.item():.4f}',
                    'è½¨è¿¹æŸå¤±': f'{traj_loss.item():.4f}'
                })
        
        avg_loss = total_loss / len(self.train_loader)
        avg_intent_loss = total_intent_loss / len(self.train_loader)
        avg_traj_loss = total_traj_loss / len(self.train_loader)
        
        self.train_losses.append(avg_loss)
        self.train_intent_losses.append(avg_intent_loss)
        self.train_traj_losses.append(avg_traj_loss)
        
        return avg_loss, avg_intent_loss, avg_traj_loss
    
    def validate(self, epoch_num=None, total_epochs=None):
        """éªŒè¯"""
        self.model.eval()
        total_loss = 0
        total_intent_loss = 0
        total_traj_loss = 0
        
        with torch.no_grad():
            with tqdm(total=len(self.val_loader), desc=f"éªŒè¯ Epoch {epoch_num}/{total_epochs}", unit="æ‰¹") as pbar:
                for batch in self.val_loader:
                    visual_feat = batch['visual_features'].to(self.device)
                    motion_feat = batch['motion_features'].to(self.device)
                    traffic_feat = batch['traffic_features'].to(self.device)
                    intent_target = batch['left_turn_intent'].to(self.device)
                    traj_target = batch['target_trajectory'].to(self.device)
                    
                    intent_pred, traj_pred = self.model(visual_feat, motion_feat, traffic_feat)
                    
                    intent_loss = self.intent_criterion(intent_pred.squeeze(), intent_target)
                    traj_loss = self.trajectory_criterion(traj_pred, traj_target)
                    total_batch_loss = 20.0 * intent_loss + 0.01 * traj_loss
                    
                    total_loss += total_batch_loss.item()
                    total_intent_loss += intent_loss.item()
                    total_traj_loss += traj_loss.item()
                    
                    pbar.update(1)
                    pbar.set_postfix({
                        'æ€»æŸå¤±': f'{total_batch_loss.item():.4f}',
                        'æ„å›¾æŸå¤±': f'{intent_loss.item():.4f}',
                        'è½¨è¿¹æŸå¤±': f'{traj_loss.item():.4f}'
                    })
        
        avg_loss = total_loss / len(self.val_loader)
        avg_intent_loss = total_intent_loss / len(self.val_loader)
        avg_traj_loss = total_traj_loss / len(self.val_loader)
        
        self.val_losses.append(avg_loss)
        self.val_intent_losses.append(avg_intent_loss)
        self.val_traj_losses.append(avg_traj_loss)
        
        return avg_loss, avg_intent_loss, avg_traj_loss
    
    def train(self, epochs: int = 50, early_stopping_patience: int = 15):
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        best_val_loss = float('inf')
        patience_counter = 0
        
        print(f"ğŸš€ å¼€å§‹è®­ç»ƒï¼Œå…± {epochs} è½®")
        print("=" * 60)
        
        for epoch in range(epochs):
            print(f"\nğŸ“ˆ Epoch {epoch + 1}/{epochs}")
            
            # è®­ç»ƒ
            train_loss, train_intent_loss, train_traj_loss = self.train_epoch(epoch + 1, epochs)
            
            # éªŒè¯
            val_loss, val_intent_loss, val_traj_loss = self.validate(epoch + 1, epochs)
            
            # å­¦ä¹ ç‡è°ƒåº¦
            self.scheduler.step(val_loss)
            
            # æ‰“å°ç»“æœ
            print(f"è®­ç»ƒæŸå¤±: {train_loss:.4f} (æ„å›¾: {train_intent_loss:.4f}, è½¨è¿¹: {train_traj_loss:.4f})")
            print(f"éªŒè¯æŸå¤±: {val_loss:.4f} (æ„å›¾: {val_intent_loss:.4f}, è½¨è¿¹: {val_traj_loss:.4f})")
            print(f"å­¦ä¹ ç‡: {self.optimizer.param_groups[0]['lr']:.6f}")
            
            # æ—©åœæ£€æŸ¥
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                patience_counter = 0
                torch.save(self.model.state_dict(), 'best_model.pth')
                print("âœ… ä¿å­˜æœ€ä½³æ¨¡å‹")
            else:
                patience_counter += 1
                if patience_counter >= early_stopping_patience:
                    print(f"ğŸ›‘ æ—©åœè§¦å‘ï¼Œå·²è¿ç»­ {early_stopping_patience} è½®æ— æ”¹å–„")
                    break
        
        print("=" * 60)
        print("ğŸ¯ è®­ç»ƒå®Œæˆï¼")
        return self.train_losses, self.val_losses
    
    def plot_training_history(self):
        """ç»˜åˆ¶è®­ç»ƒå†å²"""
        fig, axes = plt.subplots(1, 3, figsize=(15, 5))
        
        # æ€»æŸå¤±
        axes[0].plot(self.train_losses, label='è®­ç»ƒæŸå¤±')
        axes[0].plot(self.val_losses, label='éªŒè¯æŸå¤±')
        axes[0].set_title('æ€»æŸå¤±')
        axes[0].set_xlabel('Epoch')
        axes[0].set_ylabel('Loss')
        axes[0].legend()
        axes[0].grid(True)
        
        # æ„å›¾è¯†åˆ«æŸå¤±
        axes[1].plot(self.train_intent_losses, label='è®­ç»ƒæ„å›¾æŸå¤±')
        axes[1].plot(self.val_intent_losses, label='éªŒè¯æ„å›¾æŸå¤±')
        axes[1].set_title('æ„å›¾è¯†åˆ«æŸå¤±')
        axes[1].set_xlabel('Epoch')
        axes[1].set_ylabel('Loss')
        axes[1].legend()
        axes[1].grid(True)
        
        # è½¨è¿¹é¢„æµ‹æŸå¤±
        axes[2].plot(self.train_traj_losses, label='è®­ç»ƒè½¨è¿¹æŸå¤±')
        axes[2].plot(self.val_traj_losses, label='éªŒè¯è½¨è¿¹æŸå¤±')
        axes[2].set_title('è½¨è¿¹é¢„æµ‹æŸå¤±')
        axes[2].set_xlabel('Epoch')
        axes[2].set_ylabel('Loss')
        axes[2].legend()
        axes[2].grid(True)
        
        plt.tight_layout()
        plt.savefig('training_history.png', dpi=300, bbox_inches='tight')
        plt.show()


# =============================
# è½¨è¿¹å¯è§†åŒ–å‡½æ•°
# =============================
def plot_true_left_turn_samples(dataset, num_samples=5):
    """
    æµ‹è¯•å‰: ä»æ•°æ®é›†ä¸­æŠ½å–çœŸå®å·¦è½¬è®°å½•å¹¶ç»˜åˆ¶è½¨è¿¹
    
    Args:
        dataset: æ•°æ®é›†å¯¹è±¡
        num_samples: è¦ç»˜åˆ¶çš„æ ·æœ¬æ•°é‡
    """
    print(f"ğŸ¨ æµ‹è¯•å‰: æŠ½å– {num_samples} ä¸ªçœŸå®å·¦è½¬è®°å½•çš„è½¨è¿¹...")
    
    # ä»æ•°æ®é›†ä¸­æ‰¾åˆ°çœŸå®çš„å·¦è½¬æ ·æœ¬
    true_left_turn_samples = []
    
    if hasattr(dataset, 'samples'):
        # å¯¹äºMultiModalDataset
        for sample in dataset.samples:
            if sample['left_turn_intent'] > 0.5:  # å·¦è½¬æ ‡ç­¾
                true_left_turn_samples.append(sample)
                if len(true_left_turn_samples) >= num_samples * 2:  # å¤šæ”¶é›†ä¸€äº›ä»¥ä¾¿éšæœºé€‰æ‹©
                    break
    elif hasattr(dataset, 'dataset') and hasattr(dataset.dataset, 'samples'):
        # å¯¹äºSubsetåŒ…è£…çš„æ•°æ®é›†
        for i in range(len(dataset)):
            try:
                sample_data = dataset[i]
                if sample_data['left_turn_intent'].item() > 0.5:
                    # è·å–åŸå§‹æ ·æœ¬ä¿¡æ¯
                    original_idx = dataset.indices[i]
                    original_sample = dataset.dataset.samples[original_idx]
                    true_left_turn_samples.append(original_sample)
                    if len(true_left_turn_samples) >= num_samples * 2:
                        break
            except (KeyError, AttributeError, IndexError) as e:
                # å¦‚æœæ— æ³•è®¿é—®æ ·æœ¬æ•°æ®ï¼Œè·³è¿‡
                continue
    else:
        # å¯¹äºå…¶ä»–ç±»å‹çš„æ•°æ®é›†ï¼ˆå¦‚MockDatasetï¼‰ï¼Œå°è¯•ç›´æ¥è®¿é—®samples
        try:
            if hasattr(dataset, 'samples'):
                for sample in dataset.samples:
                    if sample.get('left_turn_intent', 0) > 0.5:
                        true_left_turn_samples.append(sample)
                        if len(true_left_turn_samples) >= num_samples * 2:
                            break
        except Exception as e:
            print(f"âš ï¸ æ— æ³•ä»æ•°æ®é›†æå–æ ·æœ¬: {e}")
        
        # å¦‚æœä»ç„¶æ²¡æœ‰æ‰¾åˆ°æ ·æœ¬ï¼Œåˆ›å»ºæ¨¡æ‹Ÿæ ·æœ¬
        if not true_left_turn_samples:
            print("âš ï¸ åˆ›å»ºæ¨¡æ‹Ÿå·¦è½¬æ ·æœ¬ç”¨äºå¯è§†åŒ–")
            for i in range(num_samples):
                history_traj = np.random.randn(30, 2).cumsum(axis=0)
                future_traj = np.array([[i*0.5, i*0.8] for i in range(12)])
                true_left_turn_samples.append({
                    'vehicle_id': f'mock_left_turn_{i}',
                    'history_trajectory': history_traj,
                    'future_trajectory': future_traj,
                    'left_turn_intent': 1.0
                })
    
    # å¦‚æœç»è¿‡æ‰€æœ‰å°è¯•åä»ç„¶æ²¡æœ‰æ ·æœ¬ï¼Œä¸Šé¢çš„é€»è¾‘å·²ç»åˆ›å»ºäº†æ¨¡æ‹Ÿæ ·æœ¬
    if not true_left_turn_samples:
        print("âš ï¸ æ— æ³•è·å–ä»»ä½•å·¦è½¬æ ·æœ¬")
        return
    
    # éšæœºé€‰æ‹©è¦ç»˜åˆ¶çš„æ ·æœ¬
    num_to_plot = min(num_samples, len(true_left_turn_samples))
    selected_indices = np.random.choice(len(true_left_turn_samples), num_to_plot, replace=False)
    selected_samples = [true_left_turn_samples[i] for i in selected_indices]
    
    # åˆ›å»ºå›¾å½¢
    fig, axes = plt.subplots(1, num_to_plot, figsize=(5*num_to_plot, 5))
    if num_to_plot == 1:
        axes = [axes]
    
    for i, sample in enumerate(selected_samples):
        ax = axes[i]
        
        # è·å–è½¨è¿¹æ•°æ®
        history_traj = sample['history_trajectory']
        future_traj = sample['future_trajectory']
        vehicle_id = sample['vehicle_id']
        
        # ç»˜åˆ¶å†å²è½¨è¿¹ - æ˜¾ç¤ºå®Œæ•´è·¯å¾„å’Œä¸­é—´ç‚¹
        ax.plot(history_traj[:, 0], history_traj[:, 1], 'b-', alpha=0.7, linewidth=2, label='å†å²è½¨è¿¹')
        ax.scatter(history_traj[:, 0], history_traj[:, 1], color='blue', s=30, alpha=0.6, zorder=2)  # æ˜¾ç¤ºæ‰€æœ‰å†å²ç‚¹
        
        # ç»˜åˆ¶æœªæ¥è½¨è¿¹ï¼ˆçœŸå®å·¦è½¬è½¨è¿¹ï¼‰- æ˜¾ç¤ºå®Œæ•´è·¯å¾„å’Œä¸­é—´ç‚¹
        ax.plot(future_traj[:, 0], future_traj[:, 1], 'r-', alpha=0.8, linewidth=2, label='æœªæ¥è½¨è¿¹(å·¦è½¬)')
        ax.scatter(future_traj[:, 0], future_traj[:, 1], color='red', s=30, alpha=0.6, zorder=2)  # æ˜¾ç¤ºæ‰€æœ‰æœªæ¥ç‚¹
        
        # æ ‡è®°å…³é”®ç‚¹
        ax.scatter(history_traj[0, 0], history_traj[0, 1], color='green', s=120, zorder=3, label='èµ·ç‚¹', marker='s')
        ax.scatter(future_traj[-1, 0], future_traj[-1, 1], color='red', s=120, zorder=3, label='ç»ˆç‚¹', marker='s')
        ax.scatter(history_traj[-1, 0], history_traj[-1, 1], color='orange', s=120, zorder=3, label='é¢„æµ‹èµ·ç‚¹', marker='o')
        
        # æ·»åŠ è½¨è¿¹ç‚¹æ•°ä¿¡æ¯
        ax.text(0.05, 0.85, f"å†å²ç‚¹æ•°: {len(history_traj)}", transform=ax.transAxes, 
                fontsize=9, verticalalignment='top', 
                bbox=dict(boxstyle="round,pad=0.2", facecolor="lightblue", alpha=0.7))
        ax.text(0.05, 0.75, f"æœªæ¥ç‚¹æ•°: {len(future_traj)}", transform=ax.transAxes, 
                fontsize=9, verticalalignment='top', 
                bbox=dict(boxstyle="round,pad=0.2", facecolor="lightcoral", alpha=0.7))
        
        # æ·»åŠ è½¦è¾†IDä¿¡æ¯
        ax.text(0.05, 0.95, f"è½¦è¾†ID: {vehicle_id}", transform=ax.transAxes, 
                fontsize=10, verticalalignment='top', 
                bbox=dict(boxstyle="round,pad=0.3", facecolor="lightblue", alpha=0.7))
        
        # è®¾ç½®åæ ‡è½´å’Œæ ‡é¢˜
        ax.set_title(f'çœŸå®å·¦è½¬æ ·æœ¬ {i+1}')
        ax.set_xlabel('Xåæ ‡ (m)')
        ax.set_ylabel('Yåæ ‡ (m)')
        ax.grid(True, alpha=0.3)
        ax.axis('equal')
        
        # è°ƒæ•´åæ ‡è½´èŒƒå›´
        all_x = np.concatenate([history_traj[:, 0], future_traj[:, 0]])
        all_y = np.concatenate([history_traj[:, 1], future_traj[:, 1]])
        
        x_min, x_max = np.min(all_x), np.max(all_x)
        y_min, y_max = np.min(all_y), np.max(all_y)
        
        x_margin = (x_max - x_min) * 0.1 if x_max != x_min else 10
        y_margin = (y_max - y_min) * 0.1 if y_max != y_min else 10
        
        ax.set_xlim(x_min - x_margin, x_max + x_margin)
        ax.set_ylim(y_min - y_margin, y_max + y_margin)
        
        if i == 0:  # åªåœ¨ç¬¬ä¸€ä¸ªå›¾ä¸­æ˜¾ç¤ºå›¾ä¾‹
            ax.legend(loc='upper right')
    
    plt.tight_layout()
    plt.savefig('true_left_turn_samples.png', dpi=300, bbox_inches='tight')
    plt.show()
    print("âœ… çœŸå®å·¦è½¬è½¨è¿¹æ ·ä¾‹å·²ä¿å­˜ä¸º: true_left_turn_samples.png")


def plot_predicted_left_turn_trajectories(predicted_left_turns, num_samples=5):
    """
    æµ‹è¯•å: ç»˜åˆ¶é¢„æµ‹ä¸ºå·¦è½¬çš„è½¨è¿¹æ ·ä¾‹
    
    Args:
        predicted_left_turns: é¢„æµ‹ä¸ºå·¦è½¬çš„è®°å½•åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«è½¨è¿¹ä¿¡æ¯å’Œmovementå€¼
        num_samples: è¦ç»˜åˆ¶çš„æ ·æœ¬æ•°é‡
    """
    print(f"ğŸ¨ æµ‹è¯•å: ç»˜åˆ¶ {num_samples} ä¸ªé¢„æµ‹ä¸ºå·¦è½¬çš„è½¨è¿¹æ ·ä¾‹...")
    
    if not predicted_left_turns:
        print("âš ï¸ æ²¡æœ‰é¢„æµ‹ä¸ºå·¦è½¬çš„è®°å½•å¯ä¾›å¯è§†åŒ–")
        return
    
    # é€‰æ‹©è¦ç»˜åˆ¶çš„æ ·æœ¬ï¼ˆæœ€å¤šnum_samplesä¸ªï¼‰
    num_to_plot = min(num_samples, len(predicted_left_turns))
    selected_indices = np.random.choice(len(predicted_left_turns), num_to_plot, replace=False)
    selected_samples = [predicted_left_turns[i] for i in selected_indices]
    
    # åˆ›å»ºå›¾å½¢
    fig, axes = plt.subplots(1, num_to_plot, figsize=(5*num_to_plot, 5))
    if num_to_plot == 1:
        axes = [axes]
    
    for i, sample in enumerate(selected_samples):
        ax = axes[i]
        
        # è·å–è½¨è¿¹æ•°æ®
        history_traj = sample['history_trajectory']
        predicted_traj = sample['predicted_trajectory']
        true_traj = sample.get('true_trajectory', None)
        movement_prob = sample.get('movement_prob', 0.0)
        vehicle_id = sample.get('vehicle_id', f'Vehicle_{i}')
        true_label = sample.get('true_label', 'Unknown')
        
        # ç¡®ä¿æ•°æ®æ˜¯NumPyæ•°ç»„
        if hasattr(history_traj, 'cpu'):
            history_traj = history_traj.cpu().numpy()
        if hasattr(predicted_traj, 'cpu'):
            predicted_traj = predicted_traj.cpu().numpy()
        if true_traj is not None and hasattr(true_traj, 'cpu'):
            true_traj = true_traj.cpu().numpy()
        
        # ç»˜åˆ¶å†å²è½¨è¿¹ - æ˜¾ç¤ºå®Œæ•´è·¯å¾„å’Œæ‰€æœ‰ä¸­é—´ç‚¹
        ax.plot(history_traj[:, 0], history_traj[:, 1], 'b-', alpha=0.7, linewidth=2, label='å†å²è½¨è¿¹')
        ax.scatter(history_traj[:, 0], history_traj[:, 1], color='blue', s=25, alpha=0.6, zorder=2)  # æ˜¾ç¤ºæ‰€æœ‰å†å²ç‚¹
        
        # ç»˜åˆ¶é¢„æµ‹è½¨è¿¹ - æ˜¾ç¤ºå®Œæ•´è·¯å¾„å’Œæ‰€æœ‰ä¸­é—´ç‚¹
        ax.plot(predicted_traj[:, 0], predicted_traj[:, 1], 'g--', alpha=0.8, linewidth=2, label='é¢„æµ‹è½¨è¿¹')
        ax.scatter(predicted_traj[:, 0], predicted_traj[:, 1], color='green', s=25, alpha=0.6, zorder=2)  # æ˜¾ç¤ºæ‰€æœ‰é¢„æµ‹ç‚¹
        
        # å¦‚æœæœ‰çœŸå®è½¨è¿¹ï¼Œä¹Ÿç»˜åˆ¶å‡ºæ¥ - æ˜¾ç¤ºå®Œæ•´è·¯å¾„å’Œæ‰€æœ‰ä¸­é—´ç‚¹
        if true_traj is not None:
            color = 'r' if true_label == 1 else 'orange'
            label = 'çœŸå®è½¨è¿¹(å·¦è½¬)' if true_label == 1 else 'çœŸå®è½¨è¿¹(éå·¦è½¬)'
            ax.plot(true_traj[:, 0], true_traj[:, 1], color=color, alpha=0.6, linewidth=2, label=label)
            ax.scatter(true_traj[:, 0], true_traj[:, 1], color=color, s=25, alpha=0.6, zorder=2)  # æ˜¾ç¤ºæ‰€æœ‰çœŸå®ç‚¹
        
        # æ ‡è®°å…³é”®ç‚¹
        ax.scatter(history_traj[0, 0], history_traj[0, 1], color='green', s=120, zorder=3, label='èµ·ç‚¹', marker='s')
        ax.scatter(history_traj[-1, 0], history_traj[-1, 1], color='orange', s=120, zorder=3, label='é¢„æµ‹èµ·ç‚¹', marker='o')
        ax.scatter(predicted_traj[-1, 0], predicted_traj[-1, 1], color='blue', s=120, zorder=3, label='é¢„æµ‹ç»ˆç‚¹', marker='^')
        
        if true_traj is not None:
            color = 'red' if true_label == 1 else 'darkorange'
            ax.scatter(true_traj[-1, 0], true_traj[-1, 1], color=color, s=120, zorder=3, label='çœŸå®ç»ˆç‚¹', marker='D')
        
        # æ·»åŠ ä¿¡æ¯æ–‡æœ¬
        prediction_status = "âœ“ æ­£ç¡®" if true_label == 1 else "âœ— é”™è¯¯" if true_label == 0 else "æœªçŸ¥"
        info_text = f"è½¦è¾†ID: {vehicle_id}\nå·¦è½¬æ¦‚ç‡: {movement_prob:.3f}\né¢„æµ‹çŠ¶æ€: {prediction_status}"
        
        # æ ¹æ®é¢„æµ‹æ­£ç¡®æ€§é€‰æ‹©èƒŒæ™¯è‰²
        bg_color = "lightgreen" if true_label == 1 else "lightcoral" if true_label == 0 else "lightgray"
        
        ax.text(0.05, 0.95, info_text, transform=ax.transAxes, 
                fontsize=10, verticalalignment='top', 
                bbox=dict(boxstyle="round,pad=0.3", facecolor=bg_color, alpha=0.7))
        
        # æ·»åŠ è½¨è¿¹ç‚¹æ•°ä¿¡æ¯
        ax.text(0.05, 0.75, f"å†å²ç‚¹æ•°: {len(history_traj)}", transform=ax.transAxes, 
                fontsize=9, verticalalignment='top', 
                bbox=dict(boxstyle="round,pad=0.2", facecolor="lightblue", alpha=0.7))
        ax.text(0.05, 0.65, f"é¢„æµ‹ç‚¹æ•°: {len(predicted_traj)}", transform=ax.transAxes, 
                fontsize=9, verticalalignment='top', 
                bbox=dict(boxstyle="round,pad=0.2", facecolor="lightgreen", alpha=0.7))
        if true_traj is not None:
            ax.text(0.05, 0.55, f"çœŸå®ç‚¹æ•°: {len(true_traj)}", transform=ax.transAxes, 
                    fontsize=9, verticalalignment='top', 
                    bbox=dict(boxstyle="round,pad=0.2", facecolor="lightcoral", alpha=0.7))
        
        # è®¾ç½®åæ ‡è½´
        title_suffix = " (æ­£ç¡®é¢„æµ‹)" if true_label == 1 else " (è¯¯æŠ¥)" if true_label == 0 else ""
        ax.set_title(f'é¢„æµ‹å·¦è½¬æ ·æœ¬ {i+1}{title_suffix}')
        ax.set_xlabel('Xåæ ‡ (m)')
        ax.set_ylabel('Yåæ ‡ (m)')
        ax.grid(True, alpha=0.3)
        ax.axis('equal')
        
        # è°ƒæ•´åæ ‡è½´èŒƒå›´
        all_x = np.concatenate([history_traj[:, 0], predicted_traj[:, 0]])
        all_y = np.concatenate([history_traj[:, 1], predicted_traj[:, 1]])
        
        if true_traj is not None:
            all_x = np.concatenate([all_x, true_traj[:, 0]])
            all_y = np.concatenate([all_y, true_traj[:, 1]])
        
        x_min, x_max = np.min(all_x), np.max(all_x)
        y_min, y_max = np.min(all_y), np.max(all_y)
        
        x_margin = (x_max - x_min) * 0.1 if x_max != x_min else 10
        y_margin = (y_max - y_min) * 0.1 if y_max != y_min else 10
        
        ax.set_xlim(x_min - x_margin, x_max + x_margin)
        ax.set_ylim(y_min - y_margin, y_max + y_margin)
        
        if i == 0:
            ax.legend(loc='upper right', fontsize=8)
    
    plt.tight_layout()
    plt.savefig('predicted_left_turn_samples.png', dpi=300, bbox_inches='tight')
    plt.show()
    print("âœ… é¢„æµ‹å·¦è½¬è½¨è¿¹æ ·ä¾‹å·²ä¿å­˜ä¸º: predicted_left_turn_samples.png")


# =============================
# æ¨¡å‹è¯„ä¼°å‡½æ•°
# =============================
def evaluate_model(model, test_loader, device='cuda', plot_samples=True):
    """æ¨¡å‹è¯„ä¼°"""
    model.eval()
    
    all_intent_preds = []
    all_intent_targets = []
    all_traj_preds = []
    all_traj_targets = []
    predicted_left_turns = []  # å­˜å‚¨é¢„æµ‹ä¸ºå·¦è½¬çš„è®°å½•ï¼Œç”¨äºå¯è§†åŒ–
    
    print("ğŸ“ˆ å¼€å§‹è¯„ä¼°æ¨¡å‹æ€§èƒ½...")
    
    with torch.no_grad():
        with tqdm(total=len(test_loader), desc="è¯„ä¼°è¿›åº¦", unit="æ‰¹") as pbar:
            for batch_idx, batch in enumerate(test_loader):
                visual_feat = batch['visual_features'].to(device)
                motion_feat = batch['motion_features'].to(device)
                traffic_feat = batch['traffic_features'].to(device)
                intent_target = batch['left_turn_intent'].to(device)
                traj_target = batch['target_trajectory'].to(device)
                
                # æ‰§è¡Œæ¨¡å‹é¢„æµ‹
                intent_pred, traj_pred = model(visual_feat, motion_feat, traffic_feat)
                
                # æ”¶é›†é¢„æµ‹ä¸ºå·¦è½¬çš„è®°å½•ï¼Œç”¨äºå¯è§†åŒ– - ä½¿ç”¨è¾ƒä½çš„é˜ˆå€¼ä»¥è·å¾—æ›´å¤šæ ·æœ¬
                batch_left_turn_indices = (intent_pred > 0.3).cpu().numpy().flatten()
                
                if np.any(batch_left_turn_indices):
                    sample_indices = np.where(batch_left_turn_indices)[0]
                    
                    for idx in sample_indices:
                        # è·å–è½¨è¿¹æ•°æ®
                        history_traj = motion_feat[idx].cpu().numpy()
                        pred_traj = traj_pred[idx].cpu().numpy()
                        true_traj = traj_target[idx].cpu().numpy()
                        movement_prob = intent_pred[idx].cpu().numpy().item()
                        true_label = int(intent_target[idx].cpu().numpy().item() > 0.5)
                        
                        predicted_left_turns.append({
                            'history_trajectory': history_traj,
                            'predicted_trajectory': pred_traj,
                            'true_trajectory': true_traj,
                            'movement_prob': movement_prob,
                            'true_label': true_label,
                            'vehicle_id': f'test_vehicle_{batch_idx}_{idx}'
                        })
                
                all_intent_preds.append(intent_pred.cpu().numpy())
                all_intent_targets.append(intent_target.cpu().numpy())
                all_traj_preds.append(traj_pred.cpu().numpy())
                all_traj_targets.append(traj_target.cpu().numpy())
                
                pbar.update(1)
    
    print("âœ… æ¨¡å‹è¯„ä¼°å®Œæˆï¼Œå¼€å§‹è®¡ç®—æ€§èƒ½æŒ‡æ ‡...")
    
    # åˆå¹¶ç»“æœ
    intent_preds = np.concatenate(all_intent_preds)
    intent_targets = np.concatenate(all_intent_targets)
    traj_preds = np.concatenate(all_traj_preds)
    traj_targets = np.concatenate(all_traj_targets)
    
    # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix
    from sklearn.metrics import precision_recall_curve
    
    # æ„å›¾è¯†åˆ«æŒ‡æ ‡ - å…ˆç”¨é»˜è®¤é˜ˆå€¼0.5
    intent_binary_targets = (intent_targets > 0.5).astype(int)
    
    # å°è¯•å¤šä¸ªé˜ˆå€¼ï¼Œæ‰¾åˆ°æœ€ä½³F1åˆ†æ•°çš„é˜ˆå€¼
    thresholds_to_try = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]
    best_f1 = 0
    best_threshold = 0.5
    
    for threshold in thresholds_to_try:
        temp_preds = (intent_preds > threshold).astype(int)
        temp_f1 = f1_score(intent_binary_targets, temp_preds, zero_division='warn')
        if temp_f1 > best_f1:
            best_f1 = temp_f1
            best_threshold = threshold
    
    print(f"ğŸ¯ æœ€ä½³åˆ†ç±»é˜ˆå€¼: {best_threshold} (F1={best_f1:.4f})")
    
    # ä½¿ç”¨æœ€ä½³é˜ˆå€¼è¿›è¡Œæœ€ç»ˆè¯„ä¼°
    intent_binary_preds = (intent_preds > best_threshold).astype(int)
    
    intent_accuracy = accuracy_score(intent_binary_targets, intent_binary_preds)
    intent_precision = precision_score(intent_binary_targets, intent_binary_preds, zero_division='warn')
    intent_recall = recall_score(intent_binary_targets, intent_binary_preds, zero_division='warn')
    intent_f1 = f1_score(intent_binary_targets, intent_binary_preds, zero_division='warn')
    intent_auc = roc_auc_score(intent_binary_targets, intent_preds)
    
    # æ‰“å°æ··æ·†çŸ©é˜µ
    cm = confusion_matrix(intent_binary_targets, intent_binary_preds)
    print(f"ğŸ“Š æ··æ·†çŸ©é˜µ (é˜ˆå€¼={best_threshold}):")
    print("     é¢„æµ‹")
    print("å®é™…  éå·¦è½¬  å·¦è½¬")
    print(f"éå·¦è½¬  {cm[0,0]:4d}   {cm[0,1]:4d}")
    print(f"å·¦è½¬    {cm[1,0]:4d}   {cm[1,1]:4d}")
    print(f"è¯´æ˜: TN={cm[0,0]}, FP={cm[0,1]}, FN={cm[1,0]}, TP={cm[1,1]}")
    
    # è½¨è¿¹é¢„æµ‹æŒ‡æ ‡
    ade = np.mean(np.sqrt(np.sum((traj_preds - traj_targets) ** 2, axis=2)))
    fde = np.mean(np.sqrt(np.sum((traj_preds[:, -1, :] - traj_targets[:, -1, :]) ** 2, axis=1)))
    
    # æ‰“å°è¯„ä¼°ç»“æœ
    print("=" * 60)
    print("                        æ¨¡å‹è¯„ä¼°ç»“æœ")
    print("=" * 60)
    print(f"æ„å›¾è¯†åˆ«å‡†ç¡®ç‡: {intent_accuracy:.4f}")
    print(f"æ„å›¾è¯†åˆ«ç²¾ç¡®ç‡: {intent_precision:.4f}")
    print(f"æ„å›¾è¯†åˆ«å¬å›ç‡: {intent_recall:.4f}")
    print(f"æ„å›¾è¯†åˆ«F1åˆ†æ•°: {intent_f1:.4f}")
    print(f"æ„å›¾è¯†åˆ«ROC-AUC: {intent_auc:.4f}")
    print("-" * 40)
    print(f"è½¨è¿¹é¢„æµ‹ADE: {ade:.4f} m")
    print(f"è½¨è¿¹é¢„æµ‹FDE: {fde:.4f} m")
    print("=" * 60)
    
    # å‡†å¤‡è¿”å›ç»“æœ
    results = {
        'intent_accuracy': intent_accuracy,
        'intent_precision': intent_precision,
        'intent_recall': intent_recall,
        'intent_f1': intent_f1,
        'intent_auc': intent_auc,
        'trajectory_ade': ade,
        'trajectory_fde': fde
    }
    
    # å¦‚æœplot_samplesä¸ºTrueä¸”æœ‰é¢„æµ‹ä¸ºå·¦è½¬çš„è®°å½•ï¼Œåˆ™ç»˜åˆ¶è½¨è¿¹æ ·ä¾‹
    if plot_samples and predicted_left_turns:
        plot_predicted_left_turn_trajectories(predicted_left_turns)
    
    return results


# =============================
# ä¸»å‡½æ•°
# =============================
def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ è½¦è¾†å·¦è½¬è½¨è¿¹é¢„æµ‹ç³»ç»Ÿ")
    print("åŸºäºå†å²è½¨è¿¹çš„çœŸæ­£å·¦è½¬æ„å›¾é¢„æµ‹")
    print("=" * 60)
    
    data_dir = os.path.join(os.path.dirname(__file__), "../data")
    
    # åŠ è½½é…ç½®
    config = load_config()
    
    # è®¾ç½®å‚æ•°
    history_length = config.get("history_length", 50)  # å¢åŠ åˆ°50å¸§
    raw_csv_file = config.get("raw_csv_file", "peachtree_filtered_data.csv")
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ’» ä½¿ç”¨è®¾å¤‡: {device}")
    
    # è·å–æ ·æœ¬æ•°é™åˆ¶
    max_samples_input = input("è¯·è¾“å…¥è¦å¤„ç†çš„æœ€å¤§æ ·æœ¬æ•° (é»˜è®¤: å…¨éƒ¨ï¼Œè¾“å…¥æ­£æ•´æ•°å¯ç¼©å‡è°ƒè¯•æ—¶é—´): ").strip()
    max_samples = config.get("max_samples", None)  # é»˜è®¤ä¸ºNoneï¼Œè¡¨ç¤ºå¤„ç†å…¨éƒ¨
    if max_samples_input and max_samples_input.isdigit() and int(max_samples_input) > 0:
        max_samples = int(max_samples_input)
    elif max_samples is not None and max_samples <= 0:
        max_samples = None  # ç¡®ä¿è´Ÿå€¼æˆ–0è¢«è½¬æ¢ä¸ºNone
    
    print("âœ… ä½¿ç”¨çœŸæ­£çš„é¢„æµ‹æ¨¡å¼")
    print("   - å†å²é•¿åº¦: 50å¸§ (5ç§’) - å¢åŠ å†å²é•¿åº¦ä»¥æ•æ‰æ›´å¤šå·¦è½¬æ—©æœŸç‰¹å¾")
    print("   - é¢„æµ‹èŒƒå›´: 12å¸§ (1.2ç§’)")
    print("   - åˆ©ç”¨NGSIM movementæ ‡ç­¾è¿›è¡ŒçœŸæ­£çš„é¢„æµ‹")
    if max_samples is not None and max_samples > 0:
        print(f"   - é™åˆ¶æœ€å¤§æ ·æœ¬æ•°: {max_samples}")
    else:
        print(f"   - å¤„ç†å…¨éƒ¨æ ·æœ¬ï¼ˆæ— é™åˆ¶ï¼‰")
    
    # åˆ›å»ºæ•°æ®é›†
    print("ğŸ”„ åˆ›å»ºæ•°æ®é›†...")
    try:
        # è·å–æ•°æ®è·¯å¾„
        raw_csv_file_fullpath = f"{data_dir}/{raw_csv_file}"
        data_path_input = input(f"è¯·è¾“å…¥NGSIMæ•°æ®è·¯å¾„ (é»˜è®¤: {raw_csv_file_fullpath}): ").strip()
        data_path_fullpath = data_path_input if data_path_input else raw_csv_file_fullpath
        
        # æ£€æŸ¥æ•°æ®æ˜¯å¦å­˜åœ¨
        if not os.path.exists(data_path_fullpath):
            print(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_path_fullpath}")
            print("ğŸ’¡ è¯·ç¡®ä¿NGSIMæ•°æ®æ–‡ä»¶å­˜åœ¨")
            return
        
        # åˆ›å»ºDataPipelineå®ä¾‹
        data_pipeline = DataPipeline(data_path_fullpath)
        
        # è·å–è·¯å£å’Œæ–¹å‘ä¿¡æ¯
        int_id = config.get("int_id", 1)
        approach = config.get("approach", "northbound")
        
        # é¢„æµ‹æ¨¡å¼ä¸‹å¯ä»¥é€‰æ‹©ç‰¹å®šè·¯å£å’Œæ–¹å‘
        filter_input = input("æ˜¯å¦æŒ‰è·¯å£å’Œæ–¹å‘ç­›é€‰æ•°æ®ï¼Ÿ(y/n, é»˜è®¤: y): ").strip().lower()
        filter_input = 'y' if not filter_input else filter_input
        
        if filter_input == 'y':
            try:
                # ä½¿ç”¨LeftTurnAnalyzerå‘ç°å¹¶æ˜¾ç¤ºæ•°æ®ä¸­çš„è·¯å£
                print("ğŸ” æ­£åœ¨åˆ†ææ•°æ®ä¸­çš„è·¯å£ä¿¡æ¯...")
                analyzer = LeftTurnAnalyzer(data_path_fullpath)
                analyzer.load_data()
                intersections = analyzer.discover_intersections()
                
                if intersections:
                    print("\nğŸ“‹ æ•°æ®ä¸­å‘ç°çš„è·¯å£ä¿¡æ¯:")
                    print("=" * 80)
                    print(f"{'è·¯å£ID':<8} {'æ€»è®°å½•æ•°':<12} {'è½¦è¾†æ•°':<10} {'æ–¹å‘':<20} {'æœºåŠ¨ç±»å‹':<15}")
                    print("-" * 80)
                    
                    for int_id_available, info in sorted(intersections.items()):
                        directions_str = ','.join(map(str, info['directions'][:4]))
                        movements_str = ','.join(map(str, info['movements'][:4]))
                        
                        print(f"{int_id_available:<8} {info['total_records']:<12} {info['total_vehicles']:<10} {directions_str:<20} {movements_str:<15}")
                    print("=" * 80)
                
                # è®©ç”¨æˆ·é€‰æ‹©è·¯å£ID
                int_id_input = input("è¯·è¾“å…¥è·¯å£ID (ç•™ç©ºä¸ç­›é€‰): ").strip()
                int_id = int(int_id_input) if int_id_input else None
                
                if int_id is not None:
                    approach_input = input("è¯·è¾“å…¥å…¥å£æ–¹å‘ (1-ä¸œ, 2-åŒ—, 3-è¥¿, 4-å—, ç•™ç©ºä¸ç­›é€‰): ").strip()
                    approach = int(approach_input) if approach_input and approach_input.isdigit() else None
            except Exception as e:
                print(f"âš ï¸ åˆ†æè·¯å£ä¿¡æ¯æ—¶å‡ºé”™: {e}")
        
        # è·å–è®­ç»ƒè½®æ•°
        epochs = config.get("epochs", 50)
        epochs_input = input("è¯·è¾“å…¥è®­ç»ƒè½®æ•° epochs (é»˜è®¤: 50): ").strip()
        epochs = int(epochs_input) if epochs_input else epochs
        
        # æ„å»ºæ•°æ®é›†
        build_int_id = int_id if filter_input == 'y' else None
        build_approach = approach if filter_input == 'y' else None
        
        full_dataset = data_pipeline.build_dataset(
            int_id=build_int_id,
            approach=build_approach,
            history_length=3,  # å†å²å¸§æ•°æ”¹ä¸º3
            prediction_horizon=2,  # æœªæ¥å¸§æ•°æ”¹ä¸º2
            min_trajectory_length=5,  # å¤§å¹…é™ä½åˆ°5å¸§ï¼Œè®©æ›´å¤šè½¦è¾†å‚ä¸è®­ç»ƒ
            max_samples=max_samples
        )
        
        # åˆ†ææ•°æ®é›†
        data_pipeline.get_dataset_statistics(full_dataset)
        
        # æ•°æ®é›†åˆ’åˆ†
        train_dataset, val_dataset, test_dataset = data_pipeline.split_dataset(full_dataset)
        
        # åˆ†æå„æ•°æ®é›†çš„å·¦è½¬è½¦è¾†åˆ†å¸ƒ
        data_pipeline.analyze_dataset_split(train_dataset, val_dataset, test_dataset)
        
    except Exception as e:
        print(f"âŒ æ•°æ®å¤„ç†å¤±è´¥: {e}")
        print("ğŸ’¡ è¯·æ£€æŸ¥æ•°æ®æ–‡ä»¶è·¯å¾„å’Œæ ¼å¼ï¼Œæˆ–è°ƒæ•´ç­›é€‰æ¡ä»¶")
        return
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨ - æ ¹æ®æ•°æ®é›†å¤§å°è°ƒæ•´batch_size
    train_batch_size = min(32, len(train_dataset))
    val_batch_size = min(32, len(val_dataset))
    test_batch_size = min(32, len(test_dataset))
    
    train_loader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=val_batch_size, shuffle=False)
    test_loader = DataLoader(test_dataset, batch_size=test_batch_size, shuffle=False)
    
    print(f"âœ… æ•°æ®åŠ è½½å™¨åˆ›å»ºå®Œæˆ:")
    print(f"   è®­ç»ƒæ‰¹æ¬¡å¤§å°: {train_batch_size}")
    print(f"   éªŒè¯æ‰¹æ¬¡å¤§å°: {val_batch_size}")
    print(f"   æµ‹è¯•æ‰¹æ¬¡å¤§å°: {test_batch_size}")
    
    # åˆ›å»ºæ¨¡å‹
    print("ğŸ”§ åˆ›å»ºæ¨¡å‹...")
    model = LeftTurnPredictor(prediction_horizon=2)
    
    # æµ‹è¯•å‰: æŠ½å–çœŸå®å·¦è½¬æ ·æœ¬è¿›è¡Œè½¨è¿¹å¯è§†åŒ–
    print("ğŸ¨ æµ‹è¯•å‰è½¨è¿¹å¯è§†åŒ–...")
    try:
        plot_true_left_turn_samples(test_dataset, num_samples=5)
    except Exception as e:
        print(f"âš ï¸ æµ‹è¯•å‰è½¨è¿¹å¯è§†åŒ–å¤±è´¥: {e}")
    
    # åˆ›å»ºè®­ç»ƒç®¡ç†å™¨
    trainer = TrainingManager(model, train_loader, val_loader, device)
    
    # è®­ç»ƒæ¨¡å‹
    print("ğŸš€ å¼€å§‹è®­ç»ƒ...")
    train_history, val_history = trainer.train(epochs=epochs)
    
    # ç»˜åˆ¶è®­ç»ƒå†å²
    trainer.plot_training_history()
    
    # åŠ è½½æœ€ä½³æ¨¡å‹
    model.load_state_dict(torch.load('best_model.pth'))
    
    # è¯„ä¼°æ¨¡å‹
    print("ğŸ“Š å¼€å§‹è¯„ä¼°æ¨¡å‹...")
    results = evaluate_model(model, test_loader, device)
    
    print("ğŸ‰ è®­ç»ƒå’Œè¯„ä¼°å®Œæˆï¼")


if __name__ == "__main__":
    main()
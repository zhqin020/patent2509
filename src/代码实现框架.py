#!/usr/bin/env python3
"""
è½¦è¾†å·¦è½¬è½¨è¿¹é¢„æµ‹ç³»ç»Ÿ
åŸºäºå†å²è½¨è¿¹çš„çœŸæ­£å·¦è½¬æ„å›¾é¢„æµ‹æ¡†æ¶
é‡è¦æ›´æ–°ï¼šå®ç°åŸºäºå†å²è½¨è¿¹é¢„æµ‹æœªæ¥å·¦è½¬æ„å›¾çš„çœŸæ­£é¢„æµ‹ä»»åŠ¡
"""

import sys
import os
import time
from typing import Dict, List, Tuple, Optional
import yaml
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
import cv2
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm

# å¯¼å…¥å·¦è½¬æ•°æ®åˆ†æè„šæœ¬
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
from å·¦è½¬æ•°æ®åˆ†æè„šæœ¬ import LeftTurnAnalyzer


# =============================
# é…ç½®åŠ è½½å™¨
# =============================
def load_config(config_path=None):
    # å¦‚æœæ²¡æœ‰æä¾›é…ç½®è·¯å¾„ï¼Œä½¿ç”¨è„šæœ¬æ‰€åœ¨ç›®å½•çš„config.yaml
    if config_path is None:
        script_dir = os.path.dirname(os.path.abspath(__file__))
        config_path = os.path.join(script_dir, "config.yaml")
    
    with open(config_path, "r", encoding="utf-8") as f:
        config = yaml.safe_load(f)
    return config


# =============================
# æ•°æ®ç®¡é“ç±»ï¼šç»“åˆ LeftTurnAnalyzer
# =============================
class DataPipeline:
    """æ•°æ®å¤„ç†ç®¡é“ç±»ï¼Œç”¨äºç»Ÿä¸€ç®¡ç†ä»åŸå§‹æ•°æ®åˆ°æ•°æ®é›†çš„è½¬æ¢è¿‡ç¨‹"""
    def __init__(self, raw_path):
        self.raw_path = raw_path
    
    def build_dataset(self, int_id=None, approach=None, history_length=30, prediction_horizon=12, 
                     min_trajectory_length=100, max_samples=None):
        """
        ä»åŸå§‹æ•°æ®æ„å»ºæ•°æ®é›†
        
        Args:
            int_id: è·¯å£ID
            approach: å…¥å£æ–¹å‘
            history_length: å†å²è½¨è¿¹é•¿åº¦
            prediction_horizon: é¢„æµ‹æ—¶é—´èŒƒå›´
            min_trajectory_length: æœ€å°è½¨è¿¹é•¿åº¦
            max_samples: æœ€å¤§æ ·æœ¬æ•°é™åˆ¶
        
        Returns:
            MultiModalDataset: æ„å»ºå¥½çš„æ•°æ®é›†å®ä¾‹
        """
        print("ğŸ”„ å¼€å§‹æ„å»ºæ•°æ®é›†...")
        
        # ç›´æ¥åŠ è½½åŸå§‹æ•°æ®ï¼Œé¿å…ä½¿ç”¨LeftTurnAnalyzerçš„ç­›é€‰åŠŸèƒ½
        print(f"æ­£åœ¨åŠ è½½æ•°æ®: {self.raw_path}")
        
        # åŠ è½½åŸå§‹æ•°æ®
        raw_data = pd.read_csv(self.raw_path)
        
        # æ ¹æ®ç”¨æˆ·é€‰æ‹©çš„è·¯å£å’Œæ–¹å‘è¿›è¡Œç­›é€‰ï¼Œä½†ä¿ç•™æ­£è´Ÿæ ·æœ¬
        if int_id is not None:
            # åªç­›é€‰è·¯å£ï¼Œä¸ç­›é€‰å·¦è½¬è½¦è¾†
            filtered = raw_data[raw_data['int_id'] == int_id]
            print(f"âœ… å·²è¿‡æ»¤è·¯å£ {int_id} ç›¸å…³è½¦è¾†æ•°æ®: {len(filtered)}/{len(raw_data)} æ¡è®°å½•")
            
            # å¦‚æœæŒ‡å®šäº†å…¥å£æ–¹å‘ï¼Œè¿›ä¸€æ­¥ç­›é€‰
            if approach is not None:
                filtered = filtered[filtered['direction'] == approach]
                print(f"âœ… å·²è¿‡æ»¤å…¥å£æ–¹å‘ {approach}: {len(filtered)}/{len(raw_data)} æ¡è®°å½•")
        elif approach is not None:
            # åªç­›é€‰å…¥å£æ–¹å‘ï¼Œä¸ç­›é€‰å·¦è½¬è½¦è¾†
            filtered = raw_data[raw_data['direction'] == approach]
            print(f"âœ… å·²è¿‡æ»¤å…¥å£æ–¹å‘ {approach}: {len(filtered)}/{len(raw_data)} æ¡è®°å½•")
        else:
            # ä¸è¿›è¡Œä»»ä½•ç­›é€‰
            filtered = raw_data
            print("âœ… ä½¿ç”¨å…¨éƒ¨æ•°æ®ï¼Œä¸è¿›è¡Œè·¯å£å’Œæ–¹å‘ç­›é€‰")
        
        # æ˜¾ç¤ºæ•°æ®ç»Ÿè®¡ä¿¡æ¯
        total_vehicles = len(filtered['vehicle_id'].unique())
        left_turn_vehicles = 0
        if 'movement' in filtered.columns:
            left_turn_vehicles = len(filtered[filtered['movement'] == 2]['vehicle_id'].unique())
            left_turn_percentage = (left_turn_vehicles / total_vehicles * 100) if total_vehicles > 0 else 0
            print(f"âœ… æ•°æ®å‡†å¤‡å®Œæˆ: {len(filtered)} æ¡è®°å½•, {total_vehicles} è¾†è½¦")
            print(f"   å·¦è½¬è½¦è¾†æ•°: {left_turn_vehicles} ({left_turn_percentage:.1f}%)")
        else:
            print(f"âœ… æ•°æ®å‡†å¤‡å®Œæˆ: {len(filtered)} æ¡è®°å½•, {total_vehicles} è¾†è½¦")
        
        # ç›´æ¥ä½¿ç”¨DataFrameåˆ›å»ºæ•°æ®é›†ï¼Œé¿å…ä¸´æ—¶æ–‡ä»¶
        dataset = MultiModalDataset(
            data=filtered,  # ç›´æ¥ä¼ é€’DataFrame
            history_length=history_length,
            prediction_horizon=prediction_horizon,
            min_trajectory_length=min_trajectory_length,
            max_samples=max_samples
        )
        
        return dataset
    
    def get_dataset_statistics(self, dataset):
        """è·å–æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯"""
        if hasattr(dataset, 'analyze_dataset'):
            return dataset.analyze_dataset()
        return {}
    
    def analyze_dataset_split(self, train_dataset, val_dataset, test_dataset):
        """
        åˆ†æåˆ’åˆ†åå„æ•°æ®é›†çš„å·¦è½¬è½¦è¾†åˆ†å¸ƒ
        
        Args:
            train_dataset: è®­ç»ƒé›†
            val_dataset: éªŒè¯é›†
            test_dataset: æµ‹è¯•é›†
        """
        print("\nğŸ“Š å„æ•°æ®é›†å·¦è½¬è½¦è¾†åˆ†å¸ƒç»Ÿè®¡ (æŒ‰movement):")
        
        # å®šä¹‰ç»Ÿè®¡å‡½æ•°
        def count_movement(dataset_subset):
            # è·å–åŸå§‹æ•°æ®é›†
            original_dataset = dataset_subset.dataset
            
            # æ£€æŸ¥æ˜¯å¦æœ‰sampleså±æ€§å’Œmovementä¿¡æ¯
            if hasattr(original_dataset, 'samples'):
                # å¯¹äºé¢„æµ‹æ¨¡å¼çš„æ•°æ®é›†
                total_count = 0
                left_turn_count = 0
                
                # éå†å­é›†çš„æ‰€æœ‰ç´¢å¼•
                for idx in dataset_subset.indices:
                    sample = original_dataset.samples[idx]
                    total_count += 1
                    # å‡è®¾label=1è¡¨ç¤ºå·¦è½¬
                    if sample['label'] == 1:
                        left_turn_count += 1
                
                return total_count, left_turn_count
            
            # å¯¹äºåŸå§‹æ•°æ®æ¨¡å¼
            if hasattr(original_dataset, 'raw_data'):
                # è·å–å­é›†å¯¹åº”çš„åŸå§‹æ•°æ®
                subset_data = original_dataset.raw_data.iloc[dataset_subset.indices]
                total_count = len(subset_data['vehicle_id'].unique())
                
                # ç»Ÿè®¡å·¦è½¬è½¦è¾†
                if 'movement' in subset_data.columns:
                    # å‡è®¾movement=2è¡¨ç¤ºå·¦è½¬
                    left_turn_vehicles = subset_data[subset_data['movement'] == 2]['vehicle_id'].unique()
                    left_turn_count = len(left_turn_vehicles)
                    return total_count, left_turn_count
            
            return 0, 0
        
        # ç»Ÿè®¡å„æ•°æ®é›†
        datasets = [
            (train_dataset, "è®­ç»ƒé›†"),
            (val_dataset, "éªŒè¯é›†"),
            (test_dataset, "æµ‹è¯•é›†")
        ]
        
        for dataset, name in datasets:
            total, left = count_movement(dataset)
            percentage = (left / total * 100) if total > 0 else 0
            print(f"   {name}:")
            print(f"      æ€»è½¦è¾†æ•°: {total:,}")
            print(f"      å·¦è½¬è½¦è¾†æ•°: {left:,} ({percentage:.1f}%)")
    
    def split_dataset(self, dataset, train_ratio=0.7, val_ratio=0.15):
        """
        å°†æ•°æ®é›†åˆ’åˆ†ä¸ºè®­ç»ƒé›†ã€éªŒè¯é›†å’Œæµ‹è¯•é›†
        
        Returns:
            tuple: (train_dataset, val_dataset, test_dataset)
        """
        dataset_size = len(dataset)
        train_size = int(train_ratio * dataset_size)
        val_size = int(val_ratio * dataset_size)
        test_size = dataset_size - train_size - val_size
        
        print(f"ğŸ“Š æ•°æ®é›†åˆ’åˆ†: è®­ç»ƒ={train_size}, éªŒè¯={val_size}, æµ‹è¯•={test_size}")
        
        train_dataset = torch.utils.data.Subset(dataset, range(train_size))
        val_dataset = torch.utils.data.Subset(dataset, range(train_size, train_size + val_size))
        test_dataset = torch.utils.data.Subset(dataset, range(train_size + val_size, dataset_size))
        
        return train_dataset, val_dataset, test_dataset
    

class MockDataset(Dataset):
    """æ¨¡æ‹Ÿæ•°æ®é›†ç±»ï¼Œç”¨äºæ¼”ç¤ºå’Œæµ‹è¯•"""
    def __init__(self, size=1000, history_length=30):
        self.size = size
        self.history_length = history_length
    
    def __len__(self):
        return self.size
    
    def __getitem__(self, idx):
        # ç”Ÿæˆä¸å®é™…æ•°æ®ç»´åº¦ä¸€è‡´çš„æ¨¡æ‹Ÿæ•°æ®
        # visual_features: [history_length, 32]
        # motion_features: [history_length, 6]
        # traffic_features: [history_length, 4]
        # target_trajectory: [12, 2] (é¢„æµ‹12ä¸ªç‚¹çš„è½¨è¿¹)
        return {
            'visual_features': torch.randn(self.history_length, 32),
            'motion_features': torch.randn(self.history_length, 6),
            'traffic_features': torch.randn(self.history_length, 4),
            'left_turn_intent': torch.rand(1),
            'target_trajectory': torch.randn(12, 2)
        }

class MultiModalDataset(Dataset):
    """å¤šæ¨¡æ€æ•°æ®é›†ç±» - æ”¯æŒçœŸæ­£çš„å·¦è½¬é¢„æµ‹ä»»åŠ¡"""
    
    def __init__(self, data_path: str = None, data: pd.DataFrame = None, history_length: int = 30, prediction_horizon: int = 12, 
                 min_trajectory_length: int = 100, max_samples: Optional[int] = None):
        """
        åˆå§‹åŒ–æ•°æ®é›†
        
        Args:
            data_path: æ•°æ®æ–‡ä»¶è·¯å¾„ (ä¸dataå‚æ•°äºŒé€‰ä¸€)
            data: ç›´æ¥æä¾›çš„DataFrameæ•°æ®
            history_length: å†å²è½¨è¿¹é•¿åº¦ (å¸§æ•°ï¼Œé»˜è®¤30å¸§=3ç§’)
            prediction_horizon: é¢„æµ‹æ—¶é—´èŒƒå›´ (å¸§æ•°ï¼Œé»˜è®¤12å¸§=1.2ç§’)
            min_trajectory_length: æœ€å°è½¨è¿¹é•¿åº¦è¦æ±‚
            max_samples: æœ€å¤§æ ·æœ¬æ•°é™åˆ¶ï¼Œç”¨äºç¼©å‡æµ‹è¯•è°ƒè¯•æ—¶é—´
        """
        # å‚æ•°æ£€æŸ¥
        if data is None and data_path is None:
            raise ValueError("å¿…é¡»æä¾›data_pathæˆ–dataå‚æ•°")
            
        self.data_path = data_path
        self.history_length = history_length
        self.prediction_horizon = prediction_horizon
        self.min_trajectory_length = min_trajectory_length
        self.max_samples = max_samples  
        
        # ä¸ºäº†å…¼å®¹æ€§ï¼Œä¿ç•™åŸæœ‰å‚æ•°å
        self.sequence_length = history_length
        self.prediction_length = prediction_horizon
        
        print(f"ğŸš€ åˆå§‹åŒ–æ•°æ®é›† (é¢„æµ‹æ¨¡å¼)")
        print(f"   å†å²é•¿åº¦: {history_length}å¸§ ({history_length*0.1:.1f}ç§’)")
        print(f"   é¢„æµ‹èŒƒå›´: {prediction_horizon}å¸§ ({prediction_horizon*0.1:.1f}ç§’)")
        
        # ä¼˜å…ˆä½¿ç”¨ç›´æ¥æä¾›çš„DataFrame
        if data is not None:
            self.raw_data = data
            print(f"âœ… ç›´æ¥åŠ è½½DataFrameæ•°æ®: {len(data)} æ¡è®°å½•, {len(data['vehicle_id'].unique())} è¾†è½¦")
        else:
            self.raw_data = self.load_data()
        
        # åªä½¿ç”¨é¢„æµ‹æ¨¡å¼æ„å»ºæ ·æœ¬
        self.samples = self._build_prediction_samples()
        print(f"âœ… æ„å»ºé¢„æµ‹æ ·æœ¬: {len(self.samples)} ä¸ª")
        
    def load_data(self):
        """åŠ è½½é¢„å¤„ç†å¥½çš„å·¦è½¬æ•°æ®"""
        print(f"æ­£åœ¨åŠ è½½é¢„å¤„ç†æ•°æ®: {self.data_path}")
        
        # åŠ è½½ç”±æ•°æ®é¢„å¤„ç†ç®¡é“ç”Ÿæˆçš„æ•°æ®
        data = pd.read_csv(self.data_path)
        
        print(f"æ•°æ®åŠ è½½æˆåŠŸ: {len(data)} æ¡è®°å½•, {len(data['vehicle_id'].unique())} è¾†è½¦")
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«å¿…è¦çš„åˆ—
        required_columns = ['vehicle_id', 'frame_id', 'local_x', 'local_y']
        missing_columns = [col for col in required_columns if col not in data.columns]
        if missing_columns:
            raise ValueError(f"æ•°æ®ç¼ºå°‘å¿…è¦åˆ—: {missing_columns}")
        
        # å¦‚æœæœ‰è´¨é‡æ ‡è®°ï¼Œä¼˜å…ˆä½¿ç”¨é«˜è´¨é‡æ•°æ®
        if 'is_high_quality' in data.columns:
            high_quality_data = data[data['is_high_quality'] == True]
            if len(high_quality_data) > 0:
                print(f"ä½¿ç”¨é«˜è´¨é‡æ•°æ®: {len(high_quality_data)} æ¡è®°å½•, {len(high_quality_data['vehicle_id'].unique())} è¾†è½¦")
                data = high_quality_data
        
        # æŒ‰è½¦è¾†IDå’Œå¸§IDæ’åº
        data = data.sort_values(['vehicle_id', 'frame_id'])
        
        return data
    
    def _build_prediction_samples(self):
        """æ„å»ºçœŸæ­£çš„é¢„æµ‹æ ·æœ¬"""
        samples = []
        
        # æ£€æŸ¥æ˜¯å¦æœ‰movementå­—æ®µ
        if 'movement' not in self.raw_data.columns:
            print("âš ï¸  æ•°æ®ä¸­æ²¡æœ‰movementå­—æ®µï¼Œæ— æ³•æ„å»ºé¢„æµ‹æ ·æœ¬")
            return []
        
        # æŒ‰è½¦è¾†åˆ†ç»„
        vehicle_groups = self.raw_data.groupby('vehicle_id')
        
        print("ğŸ”„ æ„å»ºé¢„æµ‹æ ·æœ¬...")
        valid_vehicles = 0
        total_vehicles = len(vehicle_groups)
        
        # ä½¿ç”¨è¿›åº¦æ¡æ˜¾ç¤ºå¤„ç†è¿›åº¦
        with tqdm(total=total_vehicles, desc="å¤„ç†è½¦è¾†æ•°æ®", unit="è¾†") as pbar:
            for vehicle_id, vehicle_data in vehicle_groups:
                # æŒ‰æ—¶é—´æ’åº
                vehicle_data = vehicle_data.sort_values('frame_id').reset_index(drop=True)
                
                # æ£€æŸ¥è½¨è¿¹é•¿åº¦
                if len(vehicle_data) < self.min_trajectory_length:
                    pbar.update(1)
                    continue
                    
                valid_vehicles += 1
                
                # æ»‘åŠ¨çª—å£æ„å»ºæ ·æœ¬
                total_length = self.history_length + self.prediction_horizon
                possible_windows = len(vehicle_data) - total_length + 1
                
                for i in range(possible_windows):
                    # å†å²è½¨è¿¹ (è¾“å…¥)
                    history_data = vehicle_data.iloc[i:i+self.history_length].copy()
                    
                    # æœªæ¥è½¨è¿¹ (ç”¨äºæ ‡ç­¾)
                    future_data = vehicle_data.iloc[i+self.history_length:i+total_length].copy()
                    
                    # æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
                    if len(history_data) != self.history_length or len(future_data) != self.prediction_horizon:
                        continue
                    
                    # æå–æ ‡ç­¾ (æœªæ¥æ˜¯å¦æœ‰å·¦è½¬)
                    future_movements = future_data['movement'].values
                    has_left_turn = np.any(future_movements == 2.0)  # 2.0 = å·¦è½¬
                    
                    # é¢å¤–ä¿¡æ¯
                    sample_info = {
                        'vehicle_id': vehicle_id,
                        'start_frame': history_data['frame_id'].iloc[0],
                        'end_frame': future_data['frame_id'].iloc[-1],
                        'history_trajectory': history_data[['local_x', 'local_y']].values,
                        'future_trajectory': future_data[['local_x', 'local_y']].values,
                        'future_movements': future_movements  # ä¿ç•™åŸå§‹movementå€¼ç”¨äºéªŒè¯
                    }
                    
                    samples.append({
                        'history_data': history_data,
                        'future_data': future_data,
                        'label': int(has_left_turn),
                        'info': sample_info
                    })
                    
                    # å¦‚æœè¾¾åˆ°æœ€å¤§æ ·æœ¬æ•°é™åˆ¶ï¼Œæå‰ç»“æŸ
                    if self.max_samples>0 and len(samples) >= self.max_samples:
                        print(f"ğŸ”„ å·²è¾¾åˆ°æœ€å¤§æ ·æœ¬æ•°é™åˆ¶ ({self.max_samples}ä¸ªæ ·æœ¬)")
                        pbar.update(total_vehicles - pbar.n)  # æ›´æ–°è¿›åº¦æ¡åˆ°100%
                        return samples[:self.max_samples]
                
                # æ›´æ–°è¿›åº¦æ¡
                pbar.update(1)
                pbar.set_postfix({
                    'æœ‰æ•ˆè½¦è¾†': valid_vehicles,
                    'æ ·æœ¬æ•°': len(samples)
                })
        
        print(f"âœ… æ„å»ºå®Œæˆï¼Œæœ‰æ•ˆè½¦è¾†: {valid_vehicles} è¾†, æ€»æ ·æœ¬æ•°: {len(samples)} ä¸ª")
        return samples
    
    def __len__(self):
        """è¿”å›æ•°æ®é›†æ ·æœ¬æ•°é‡"""
        return len(self.samples)
    
    def __getitem__(self, idx):
        """è·å–å•ä¸ªé¢„æµ‹æ ·æœ¬"""
        return self._get_prediction_sample(idx)
    
    def _get_prediction_sample(self, idx):
        """è·å–é¢„æµ‹æ ·æœ¬"""
        if idx >= len(self.samples):
            idx = idx % len(self.samples)
        
        sample = self.samples[idx]
        history_data = sample['history_data']
        future_data = sample['future_data']
        label = sample['label']
        
        # æå–å¤šæ¨¡æ€ç‰¹å¾ï¼ˆåŸºäºå†å²æ•°æ®ï¼‰
        visual_features = self.extract_visual_features(history_data)
        motion_features = self.extract_motion_features(history_data)
        traffic_features = self.extract_traffic_features(history_data)
        
        # ç›®æ ‡è½¨è¿¹ï¼ˆæœªæ¥è½¨è¿¹ï¼‰
        target_trajectory = future_data[['local_x', 'local_y']].values
        
        # æå–æœªæ¥movementå­—æ®µä½œä¸ºæœªæ¥å€¼
        future_movements = sample['info'].get('future_movements', np.array([]))
        
        return {
            'visual_features': torch.FloatTensor(visual_features),
            'motion_features': torch.FloatTensor(motion_features),
            'traffic_features': torch.FloatTensor(traffic_features),
            'left_turn_intent': torch.FloatTensor([float(label)]),  # çœŸæ­£çš„é¢„æµ‹æ ‡ç­¾
            'target_trajectory': torch.FloatTensor(target_trajectory),
            'future_movements': torch.FloatTensor(future_movements),  # æ·»åŠ æœªæ¥movementå­—æ®µä½œä¸ºæœªæ¥å€¼
            'sample_info': sample['info']  # é¢å¤–ä¿¡æ¯
        }
    

    
    def extract_visual_features(self, history):
        """
        æå–è§†è§‰ç‰¹å¾ã€‚ç”±äºNGSIMæ²¡æœ‰å›¾åƒï¼Œè¿™é‡Œå°†è½¦è¾†è½¨è¿¹å’Œå‘¨å›´ç¯å¢ƒä¿¡æ¯æ …æ ¼åŒ–ä¸ºç‰¹å¾è¡¨ç¤ºã€‚
        
        ä¸»è¦æ”¹è¿›ï¼š
        1. å°†è½¦è¾†è‡ªèº«è½¨è¿¹æ …æ ¼åŒ–
        2. æå–å‘¨å›´è½¦è¾†çš„ç›¸å¯¹ä½ç½®å’Œè¿åŠ¨ä¿¡æ¯
        3. è®¡ç®—è½¦è¾†ä¸å‘¨å›´ç¯å¢ƒçš„ç©ºé—´å…³ç³»ç‰¹å¾
        4. ä¸å†è¿”å›éšæœºå¼ é‡ï¼Œè€Œæ˜¯æœ‰æ„ä¹‰çš„ç‰¹å¾è¡¨ç¤º
        5. æ·»åŠ å¯¹headingåˆ—çš„å­˜åœ¨æ€§æ£€æŸ¥ï¼Œç¡®ä¿å…¼å®¹æ€§
        """
        # ä»å†å²æ•°æ®ä¸­æå–è‡ªèº«è½¦è¾†è½¨è¿¹ï¼Œæ£€æŸ¥headingåˆ—æ˜¯å¦å­˜åœ¨
        required_columns = ['local_x', 'local_y', 'v_vel', 'v_acc']
        available_columns = [col for col in required_columns if col in history.columns]
        
        # å¦‚æœheadingåˆ—å­˜åœ¨ï¼Œä¹ŸåŒ…å«å®ƒ
        if 'heading' in history.columns:
            available_columns.append('heading')
        
        # æå–å¯ç”¨åˆ—çš„æ•°æ®
        ego_trajectory = history[available_columns].values.astype(np.float32)
        
        # å¦‚æœç¼ºå°‘headingåˆ—ï¼Œæ·»åŠ é»˜è®¤å€¼0
        if 'heading' not in history.columns:
            # ä¸ºæ¯æ¡è½¨è¿¹æ·»åŠ 0å€¼çš„headingåˆ—
            num_rows = ego_trajectory.shape[0]
            default_heading = np.zeros((num_rows, 1), dtype=np.float32)
            ego_trajectory = np.hstack([ego_trajectory, default_heading])
        
        # è·å–å½“å‰æ—¶é—´æ­¥çš„ä½ç½®ä½œä¸ºå‚è€ƒç‚¹
        current_position = ego_trajectory[-1, :2] if len(ego_trajectory) > 0 else np.zeros(2)
        
        # 1. è®¡ç®—è½¨è¿¹å½¢çŠ¶ç‰¹å¾
        trajectory_features = []
        if len(ego_trajectory) > 1:
            # è®¡ç®—è½¨è¿¹æ›²ç‡ç‰¹å¾
            for i in range(1, len(ego_trajectory)):
                # è®¡ç®—ä¸å‰ä¸€å¸§çš„ä½ç§»
                displacement = ego_trajectory[i, :2] - ego_trajectory[i-1, :2]
                displacement_norm = np.linalg.norm(displacement) if np.linalg.norm(displacement) > 0 else 1
                
                # æå–æ ‡å‡†åŒ–çš„ä½ç§»å’Œé€Ÿåº¦ä¿¡æ¯
                traj_feature = np.concatenate([
                    displacement / displacement_norm,  # æ ‡å‡†åŒ–ä½ç§»æ–¹å‘
                    [ego_trajectory[i, 2] / 30],  # æ ‡å‡†åŒ–é€Ÿåº¦ (å‡è®¾æœ€å¤§é€Ÿåº¦ä¸º30 m/s)
                    [ego_trajectory[i, 3] / 5],   # æ ‡å‡†åŒ–åŠ é€Ÿåº¦ (å‡è®¾æœ€å¤§åŠ é€Ÿåº¦ä¸º5 m/s^2)
                    [np.sin(ego_trajectory[i, 4]), np.cos(ego_trajectory[i, 4])]  # èˆªå‘è§’çš„æ­£å¼¦å’Œä½™å¼¦
                ])
                trajectory_features.append(traj_feature)
        
        # å¦‚æœæ²¡æœ‰è¶³å¤Ÿçš„è½¨è¿¹ç‚¹ï¼Œç”¨é›¶å¡«å……
        while len(trajectory_features) < self.history_length:
            trajectory_features.append(np.zeros(6))  # 6ä¸ªè½¨è¿¹ç‰¹å¾
        
        # 2. æå–å‘¨å›´è½¦è¾†ä¿¡æ¯ç‰¹å¾ (å¦‚æœæœ‰)
        # å‡è®¾historyä¸­åŒ…å«å‘¨å›´è½¦è¾†ä¿¡æ¯ï¼ˆå¦‚é‚»è½¦ä½ç½®ï¼‰
        # è¿™é‡Œæˆ‘ä»¬æ¨¡æ‹Ÿä¸€äº›å…³é”®ç‰¹å¾
        surrounding_features = []
        
        # æå–ä¸å‘¨å›´ç¯å¢ƒç›¸å…³çš„ç‰¹å¾
        # å‡è®¾æˆ‘ä»¬æƒ³çŸ¥é“è½¦è¾†æ˜¯å¦é è¿‘è·¯å£ã€é“è·¯è¾¹ç•Œç­‰
        # ä¸ºäº†ç®€åŒ–ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸€äº›åŸºäºè½¨è¿¹çš„å¯å‘å¼ç‰¹å¾
        
        # è®¡ç®—è½¦è¾†æ˜¯å¦åœ¨å‡é€Ÿï¼ˆå¯èƒ½æ¥è¿‘è·¯å£ï¼‰
        is_decelerating = 1.0 if len(ego_trajectory) > 1 and ego_trajectory[-1, 3] < -0.5 else 0.0
        
        # è®¡ç®—è½¦è¾†æ˜¯å¦æœ‰è½¬å‘è¶‹åŠ¿
        has_turning_trend = 0.0
        if len(ego_trajectory) > 5 and ego_trajectory.shape[1] >= 5:
            # ä½¿ç”¨headingåˆ—çš„æœ€åä¸€ä¸ªä½ç½®ï¼ˆå› ä¸ºæˆ‘ä»¬åœ¨ä¿®å¤ä»£ç ä¸­å¯èƒ½åœ¨æœ€åæ·»åŠ äº†headingåˆ—ï¼‰
            heading_col_index = -1  # æœ€åä¸€åˆ—å§‹ç»ˆæ˜¯headingæ•°æ®
            recent_headings = ego_trajectory[-5:, heading_col_index]
            heading_diff = np.abs(recent_headings[-1] - recent_headings[0])
            has_turning_trend = 1.0 if heading_diff > 0.1 else 0.0
        
        # 3. æ„å»ºæœ€ç»ˆçš„è§†è§‰ç‰¹å¾è¡¨ç¤º
        # ç¡®ä¿ç‰¹å¾ç»´åº¦ä¸º32ï¼ˆä¸VisualEncoderçš„æœŸæœ›è¾“å…¥ä¸€è‡´ï¼‰
        # å°†è½¨è¿¹ç‰¹å¾å’Œç¯å¢ƒç‰¹å¾æŠ•å½±åˆ°32ç»´ç©ºé—´
        visual_features = []
        for traj_feat in trajectory_features:
            # æ¯ä¸ªæ—¶é—´æ­¥çš„ç‰¹å¾
            time_step_features = np.zeros(32)
            
            # å¡«å……è½¨è¿¹ç‰¹å¾ (6ä¸ªç‰¹å¾)
            time_step_features[:6] = traj_feat
            
            # å¡«å……ç¯å¢ƒç›¸å…³ç‰¹å¾
            time_step_features[6] = is_decelerating
            time_step_features[7] = has_turning_trend
            
            # åœ¨å‰©ä½™ç»´åº¦ä¸Šç¼–ç è½¦è¾†çš„ç©ºé—´ä½ç½®ä¿¡æ¯
            # å°†å½“å‰ä½ç½®ç¼–ç ä¸ºç‰¹å¾
            pos_encoded = np.zeros(24)  # å‰©ä½™24ä¸ªç»´åº¦
            pos_encoded[:2] = current_position / 100  # æ ‡å‡†åŒ–ä½ç½®
            pos_encoded[2] = ego_trajectory[-1, 2] if len(ego_trajectory) > 0 else 0.0  # é€Ÿåº¦
            
            # æ·»åŠ ä¸€äº›åŸºäºæ­£å¼¦å’Œä½™å¼¦çš„ä½ç½®ç¼–ç ï¼Œä»¥æ•è·ç©ºé—´å…³ç³»
            # ä¿®æ”¹å¾ªç¯æ¡ä»¶ä¸ºi < 22ï¼Œç¡®ä¿i+1ä¸ä¼šè¶…å‡ºæ•°ç»„è¾¹ç•Œ
            for i in range(3, 22, 2):
                freq = 0.1 * (i // 2)
                pos_encoded[i] = np.sin(freq * current_position[0])
                pos_encoded[i+1] = np.cos(freq * current_position[1])
            
            time_step_features[8:] = pos_encoded
            
            visual_features.append(time_step_features)
        
        return np.array(visual_features)
    
    def extract_motion_features(self, history):
        """
        ä»è½¦è¾†è½¨è¿¹æ•°æ®ä¸­æå–è¿åŠ¨å­¦ç‰¹å¾ï¼Œå¹¶è®¡ç®—å¯¼æ•°ã€‚
        ç‰¹å¾åŒ…æ‹¬ï¼šx, y, é€Ÿåº¦, åŠ é€Ÿåº¦, èˆªå‘è§’, èˆªå‘è§’å˜åŒ–ç‡ã€‚
        """
        # æå–åŸºç¡€è¿åŠ¨å­¦ç‰¹å¾
        trajectory_data = history[['local_x', 'local_y', 'v_vel', 'v_acc']].values.astype(np.float32)

        # ç¡®ä¿æ•°æ®å¸§è¶³å¤Ÿé•¿ä»¥è®¡ç®—å¯¼æ•°
        if len(trajectory_data) < 2:
            # å¦‚æœè½¨è¿¹å¤ªçŸ­ï¼Œè¿”å›ä¸€ä¸ªå¡«å……äº†0çš„å¼ é‡
            return np.zeros((self.history_length, 6), dtype=np.float32)

        # è®¡ç®—èˆªå‘è§’ï¼ˆheading angleï¼‰ï¼Œä½¿ç”¨atan2æ¥å¤„ç†æ‰€æœ‰è±¡é™
        dx = np.diff(trajectory_data[:, 0])
        dy = np.diff(trajectory_data[:, 1])
        # ä½¿ç”¨np.arctan2è®¡ç®—èˆªå‘è§’ï¼Œå¯ä»¥å¤„ç†æ‰€æœ‰è±¡é™
        heading = np.arctan2(dy, dx)
        heading = np.insert(heading, 0, heading[0] if len(heading) > 0 else 0)  # åœ¨ç¬¬ä¸€å¸§è¡¥ä¸Šä¸€ä¸ªå€¼

        # è®¡ç®—èˆªå‘è§’å˜åŒ–ç‡ï¼ˆheading rate of changeï¼‰
        # è€ƒè™‘è§’åº¦çš„å‘¨æœŸæ€§ï¼Œä½¿ç”¨np.unwrapæ¥é¿å…è·³å˜
        unwrapped_heading = np.unwrap(heading)
        heading_rate = np.diff(unwrapped_heading)
        heading_rate = np.insert(heading_rate, 0, 0)  # ç¬¬ä¸€å¸§å˜åŒ–ç‡ä¸º0

        # å°†æ‰€æœ‰ç‰¹å¾æ‹¼æ¥èµ·æ¥
        motion_features = np.stack([
            trajectory_data[:, 0],  # local_x
            trajectory_data[:, 1],  # local_y
            trajectory_data[:, 2],  # v_vel
            trajectory_data[:, 3],  # v_acc
            heading,               # èˆªå‘è§’
            heading_rate           # èˆªå‘è§’å˜åŒ–ç‡
        ], axis=1)

        return motion_features
    
    def extract_traffic_features(self, history):
        """
        æå–äº¤é€šç¯å¢ƒç‰¹å¾ï¼Œå¦‚ä¸å‰åè½¦çš„ç›¸å¯¹è·ç¦»å’Œç›¸å¯¹é€Ÿåº¦ã€‚
        """
        # è·å–å½“å‰è½¦è¾†çš„ID
        current_vehicle_id = history['vehicle_id'].iloc[0]

        # æå–å‰åè½¦ID
        preceding_id = history['preceding'].iloc[0]
        following_id = history['following'].iloc[0]

        # é»˜è®¤ç‰¹å¾ï¼Œå¦‚æœå‰åè½¦ä¸å­˜åœ¨ï¼Œåˆ™ä¸º0
        preceding_features = np.zeros(2)
        following_features = np.zeros(2)

        # æŸ¥æ‰¾å‰è½¦å¹¶è®¡ç®—ç›¸å¯¹ç‰¹å¾
        if preceding_id > 0:
            preceding_vehicle_df = self.raw_data[self.raw_data['vehicle_id'] == preceding_id]
            if not preceding_vehicle_df.empty:
                # æ‰¾åˆ°åŒä¸€å¸§çš„å‰è½¦æ•°æ®
                current_frame = history['frame_id'].iloc[0]
                preceding_frame_df = preceding_vehicle_df[preceding_vehicle_df['frame_id'] == current_frame]
                if not preceding_frame_df.empty:
                    preceding_pos = preceding_frame_df['local_y'].iloc[0]
                    preceding_vel = preceding_frame_df['v_vel'].iloc[0]
                    
                    # ç›¸å¯¹ä½ç½®å’Œç›¸å¯¹é€Ÿåº¦
                    relative_y = preceding_pos - history['local_y'].iloc[0]
                    relative_v = preceding_vel - history['v_vel'].iloc[0]
                    preceding_features = np.array([relative_y, relative_v], dtype=np.float32)

        # æŸ¥æ‰¾åè½¦å¹¶è®¡ç®—ç›¸å¯¹ç‰¹å¾
        if following_id > 0:
            following_vehicle_df = self.raw_data[self.raw_data['vehicle_id'] == following_id]
            if not following_vehicle_df.empty:
                # æ‰¾åˆ°åŒä¸€å¸§çš„åè½¦æ•°æ®
                current_frame = history['frame_id'].iloc[0]
                following_frame_df = following_vehicle_df[following_vehicle_df['frame_id'] == current_frame]
                if not following_frame_df.empty:
                    following_pos = following_frame_df['local_y'].iloc[0]
                    following_vel = following_frame_df['v_vel'].iloc[0]

                    # ç›¸å¯¹ä½ç½®å’Œç›¸å¯¹é€Ÿåº¦
                    relative_y = following_pos - history['local_y'].iloc[0]
                    relative_v = following_vel - history['v_vel'].iloc[0]
                    following_features = np.array([relative_y, relative_v], dtype=np.float32)

        # å°†æ‰€æœ‰äº¤é€šç‰¹å¾æ‹¼æ¥èµ·æ¥
        traffic_features = np.concatenate([preceding_features, following_features])
        
        # æ‰©å±•ä¸ºæ—¶é—´åºåˆ—æ ¼å¼
        traffic_features = np.tile(traffic_features, (self.history_length, 1))
        
        return traffic_features
    
    def get_left_turn_intent(self, vehicle_data):
        """ä»æ•°æ®ä¸­è·å–å·¦è½¬æ„å›¾æ ‡ç­¾"""
        # ä¼˜å…ˆä½¿ç”¨é¢„å¤„ç†æ•°æ®çš„è´¨é‡æ ‡è®°
        if 'is_high_quality' in vehicle_data.columns:
            return 1.0 if vehicle_data['is_high_quality'].iloc[0] else 0.0
        # å…¶æ¬¡ä½¿ç”¨NGSIMåŸå§‹movementæ ‡ç­¾
        elif 'movement' in vehicle_data.columns:
            # æ£€æŸ¥æ˜¯å¦æœ‰å·¦è½¬æ ‡ç­¾
            has_left_turn = np.any(vehicle_data['movement'] == 2.0)
            return 1.0 if has_left_turn else 0.0
        else:
            # å…¼å®¹æ€§å¤„ç†ï¼šé»˜è®¤ä¸ºå·¦è½¬æ•°æ®
            return 1.0
    
    def analyze_dataset(self):
        """åˆ†ææ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯ï¼ŒåŒ…æ‹¬é€Ÿåº¦ã€åŠ é€Ÿåº¦ã€èˆªå‘è§’ç­‰å…³é”®ç‰¹å¾ç»Ÿè®¡"""
        if hasattr(self, 'samples'):
            print(f"ğŸ“Š é¢„æµ‹æ•°æ®é›†åˆ†æ:")
            print(f"   æ€»æ ·æœ¬æ•°: {len(self.samples):,}")
            
            # ä½¿ç”¨è¿›åº¦æ¡è¿›è¡Œæ•°æ®åˆ†æ
            with tqdm(total=5, desc="åˆ†æè¿›åº¦", unit="é¡¹") as pbar:
                # æ ‡ç­¾åˆ†å¸ƒ
                labels = [sample['label'] for sample in self.samples]
                left_turn_count = sum(labels)
                non_left_turn_count = len(labels) - left_turn_count
                
                print(f"   å·¦è½¬æ ·æœ¬: {left_turn_count:,} ({left_turn_count/len(labels)*100:.1f}%)")
                print(f"   éå·¦è½¬æ ·æœ¬: {non_left_turn_count:,} ({non_left_turn_count/len(labels)*100:.1f}%)")
                pbar.update(1)
                
                # è½¦è¾†åˆ†å¸ƒ
                vehicle_ids = [sample['info']['vehicle_id'] for sample in self.samples]
                unique_vehicles = len(set(vehicle_ids))
                print(f"   æ¶‰åŠè½¦è¾†æ•°: {unique_vehicles:,}")
                print(f"   å¹³å‡æ¯è½¦æ ·æœ¬æ•°: {len(self.samples)/unique_vehicles:.1f}")
                pbar.update(1)
                
                # æ—¶é—´è·¨åº¦åˆ†æ
                if len(self.samples) > 0:
                    sample_info = self.samples[0]['info']
                    time_span = sample_info['end_frame'] - sample_info['start_frame']
                    print(f"   æ ·æœ¬æ—¶é—´è·¨åº¦: {time_span}å¸§ ({time_span*0.1:.1f}ç§’)")
                pbar.update(1)
                
                # ä½ç½®èŒƒå›´åˆ†æ
                if len(self.samples) > 0:
                    all_positions = []
                    for sample in self.samples[:1000]:  # åªåˆ†æå‰1000ä¸ªæ ·æœ¬ä»¥åŠ é€Ÿ
                        all_positions.extend(sample['info']['history_trajectory'])
                        all_positions.extend(sample['info']['future_trajectory'])
                    
                    if all_positions:
                        positions_array = np.array(all_positions)
                        min_x, min_y = np.min(positions_array, axis=0)
                        max_x, max_y = np.max(positions_array, axis=0)
                        print(f"   è½¨è¿¹ç©ºé—´èŒƒå›´: x[{min_x:.1f}, {max_x:.1f}], y[{min_y:.1f}, {max_y:.1f}]")
                pbar.update(1)
                
                # å…³é”®ç‰¹å¾ç»Ÿè®¡åˆ†æ (åªåˆ†æå‰1000ä¸ªæ ·æœ¬ä»¥æé«˜æ•ˆç‡)
                if len(self.samples) > 0:
                    print("   å…³é”®è¿åŠ¨ç‰¹å¾ç»Ÿè®¡åˆ†æ:")
                    # å­˜å‚¨æ‰€æœ‰æ ·æœ¬çš„é€Ÿåº¦ã€åŠ é€Ÿåº¦å’Œèˆªå‘è§’
                    speeds = []
                    accelerations = []
                    headings = []
                    
                    # åˆ†æå‰1000ä¸ªæ ·æœ¬
                    sample_limit = min(1000, len(self.samples))
                    for i in range(sample_limit):
                        sample = self.samples[i]
                        history_data = sample['history_data']
                        
                        # æå–é€Ÿåº¦ç‰¹å¾
                        if 'v_vel' in history_data.columns:
                            speeds.extend(history_data['v_vel'].values)
                        
                        # æå–åŠ é€Ÿåº¦ç‰¹å¾
                        if 'v_acc' in history_data.columns:
                            accelerations.extend(history_data['v_acc'].values)
                        
                        # è®¡ç®—èˆªå‘è§’ (å¦‚æœæ•°æ®ä¸­æ²¡æœ‰ç›´æ¥æä¾›)
                        if 'local_x' in history_data.columns and 'local_y' in history_data.columns:
                            positions = history_data[['local_x', 'local_y']].values
                            if len(positions) >= 2:
                                dx = np.diff(positions[:, 0])
                                dy = np.diff(positions[:, 1])
                                heading = np.arctan2(dy, dx)  # å¼§åº¦åˆ¶
                                headings.extend(heading)
                    
                    # è¾“å‡ºé€Ÿåº¦ç»Ÿè®¡
                    if speeds:
                        speeds = np.array(speeds)
                        print(f"     é€Ÿåº¦ç»Ÿè®¡: å‡å€¼={np.mean(speeds):.2f} m/s, æ ‡å‡†å·®={np.std(speeds):.2f} m/s, ")
                        print(f"              æœ€å°å€¼={np.min(speeds):.2f} m/s, æœ€å¤§å€¼={np.max(speeds):.2f} m/s")
                    
                    # è¾“å‡ºåŠ é€Ÿåº¦ç»Ÿè®¡
                    if accelerations:
                        accelerations = np.array(accelerations)
                        print(f"     åŠ é€Ÿåº¦ç»Ÿè®¡: å‡å€¼={np.mean(accelerations):.2f} m/sÂ², æ ‡å‡†å·®={np.std(accelerations):.2f} m/sÂ², ")
                        print(f"                æœ€å°å€¼={np.min(accelerations):.2f} m/sÂ², æœ€å¤§å€¼={np.max(accelerations):.2f} m/sÂ²")
                    
                    # è¾“å‡ºèˆªå‘è§’ç»Ÿè®¡
                    if headings:
                        headings = np.array(headings)
                        # è½¬æ¢ä¸ºè§’åº¦åˆ¶ä»¥ä¾¿äºç†è§£
                        headings_deg = np.rad2deg(headings)
                        print(f"     èˆªå‘è§’ç»Ÿè®¡: å‡å€¼={np.mean(headings_deg):.2f}Â°, æ ‡å‡†å·®={np.std(headings_deg):.2f}Â°, ")
                        print(f"                æœ€å°å€¼={np.min(headings_deg):.2f}Â°, æœ€å¤§å€¼={np.max(headings_deg):.2f}Â°")
                pbar.update(1)
        else:
            print(f"ğŸ“Š ä¼ ç»Ÿæ•°æ®é›†åˆ†æ:")
            print(f"   æ•°æ®è®°å½•æ•°: {len(self.data):,}")
            
            with tqdm(total=3, desc="åˆ†æè¿›åº¦", unit="é¡¹") as pbar:
                # è½¦è¾†ç»Ÿè®¡
                print(f"   è½¦è¾†æ•°: {len(self.data['vehicle_id'].unique()):,}")
                pbar.update(1)
                
                # ç§»åŠ¨ç±»å‹ç»Ÿè®¡
                if 'movement' in self.data.columns:
                    movement_counts = self.data['movement'].value_counts()
                    print(f"   Movementåˆ†å¸ƒ:")
                    for movement, count in movement_counts.items():
                        movement_name = {1.0: 'ç›´è¡Œ', 2.0: 'å·¦è½¬', 3.0: 'å³è½¬'}.get(movement, f'å…¶ä»–({movement})')
                        print(f"     {movement_name}: {count:,} ({count/len(self.data)*100:.1f}%)")
                pbar.update(1)
                
                # å…³é”®è¿åŠ¨ç‰¹å¾ç»Ÿè®¡åˆ†æ
                print("   å…³é”®è¿åŠ¨ç‰¹å¾ç»Ÿè®¡åˆ†æ:")
                
                # é€Ÿåº¦ç»Ÿè®¡
                if 'v_vel' in self.data.columns:
                    speeds = self.data['v_vel'].values
                    if len(speeds) > 0:
                        speeds = np.array(speeds)
                        print(f"     é€Ÿåº¦ç»Ÿè®¡: å‡å€¼={np.mean(speeds):.2f} m/s, æ ‡å‡†å·®={np.std(speeds):.2f} m/s, ")
                        print(f"              æœ€å°å€¼={np.min(speeds):.2f} m/s, æœ€å¤§å€¼={np.max(speeds):.2f} m/s")
                
                # åŠ é€Ÿåº¦ç»Ÿè®¡
                if 'v_acc' in self.data.columns:
                    accelerations = self.data['v_acc'].values
                    if len(accelerations) > 0:
                        accelerations = np.array(accelerations)
                        print(f"     åŠ é€Ÿåº¦ç»Ÿè®¡: å‡å€¼={np.mean(accelerations):.2f} m/sÂ², æ ‡å‡†å·®={np.std(accelerations):.2f} m/sÂ², ")
                        print(f"                æœ€å°å€¼={np.min(accelerations):.2f} m/sÂ², æœ€å¤§å€¼={np.max(accelerations):.2f} m/sÂ²")
                
                # èˆªå‘è§’ç»Ÿè®¡ (åŸºäºä½ç½®æ•°æ®è®¡ç®—)
                if 'local_x' in self.data.columns and 'local_y' in self.data.columns and 'vehicle_id' in self.data.columns:
                    # ä¸ºæ¯è¾†è½¦è®¡ç®—èˆªå‘è§’
                    headings = []
                    
                    # æŒ‰è½¦è¾†åˆ†ç»„å¤„ç†
                    vehicle_groups = self.data.groupby('vehicle_id')
                    
                    # é™åˆ¶åˆ†æçš„è½¦è¾†æ•°é‡ä»¥æé«˜æ€§èƒ½
                    max_vehicles = 500  # æœ€å¤šåˆ†æ500è¾†è½¦
                    vehicle_count = 0
                    
                    for vehicle_id, vehicle_data in vehicle_groups:
                        if vehicle_count >= max_vehicles:
                            break
                        
                        # æŒ‰æ—¶é—´æ’åº
                        sorted_data = vehicle_data.sort_values('frame_id')
                        positions = sorted_data[['local_x', 'local_y']].values
                        
                        # è®¡ç®—èˆªå‘è§’
                        if len(positions) >= 2:
                            dx = np.diff(positions[:, 0])
                            dy = np.diff(positions[:, 1])
                            heading = np.arctan2(dy, dx)  # å¼§åº¦åˆ¶
                            headings.extend(heading)
                        
                        vehicle_count += 1
                    
                    if headings:
                        headings = np.array(headings)
                        headings_deg = np.rad2deg(headings)
                        print(f"     èˆªå‘è§’ç»Ÿè®¡: å‡å€¼={np.mean(headings_deg):.2f}Â°, æ ‡å‡†å·®={np.std(headings_deg):.2f}Â°, ")
                        print(f"                æœ€å°å€¼={np.min(headings_deg):.2f}Â°, æœ€å¤§å€¼={np.max(headings_deg):.2f}Â°")
                
                pbar.update(1)

class VisualEncoder(nn.Module):
    """è§†è§‰ç‰¹å¾ç¼–ç å™¨"""
    
    def __init__(self, input_dim: int = 32, hidden_dim: int = 128):
        super().__init__()
        # è§†è§‰ç‰¹å¾æ˜¯åºåˆ—å½¢å¼ [batch_size, history_length, input_dim]
        # ä½¿ç”¨LSTMå¤„ç†åºåˆ—ç‰¹å¾
        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)
        self.dropout = nn.Dropout(0.2)
        
        # è¾“å‡ºå±‚
        self.out_layer = nn.Linear(hidden_dim, hidden_dim)
        self.relu = nn.ReLU()
    
    def forward(self, x):
        # x: [batch_size, history_length, input_dim]
        # ä½¿ç”¨LSTMå¤„ç†åºåˆ—ç‰¹å¾
        lstm_out, _ = self.lstm(x)
        # å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å‡º
        last_out = lstm_out[:, -1, :]
        last_out = self.dropout(last_out)
        
        # é€šè¿‡è¾“å‡ºå±‚
        out = self.out_layer(last_out)
        out = self.relu(out)
        
        return out

class MotionEncoder(nn.Module):
    """è¿åŠ¨ç‰¹å¾ç¼–ç å™¨ - å¤„ç†åºåˆ—å½¢å¼çš„è¿åŠ¨å­¦ç‰¹å¾"""
    
    def __init__(self, num_features: int = 6, hidden_dim: int = 128, num_layers: int = 2):
        super().__init__()
        # ç›´æ¥ä½¿ç”¨è¿åŠ¨å­¦ç‰¹å¾çš„æ•°é‡ä½œä¸ºè¾“å…¥ç»´åº¦
        # å‡è®¾è¾“å…¥æ˜¯ [batch_size, history_length, num_features] æ ¼å¼
        self.lstm = nn.LSTM(
            input_size=num_features,
            hidden_size=hidden_dim,
            num_layers=num_layers,
            batch_first=True,
            dropout=0.2
        )
        self.fc = nn.Linear(hidden_dim, hidden_dim)
    
    def forward(self, x):
        # x: [batch_size, history_length, num_features]
        # LSTMç›´æ¥å¤„ç†åºåˆ—ç‰¹å¾
        output, (hidden, cell) = self.lstm(x)
        # ä½¿ç”¨æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å‡º
        return self.fc(output[:, -1, :])

class TrafficEncoder(nn.Module):
    """äº¤é€šç¯å¢ƒç‰¹å¾ç¼–ç å™¨ - å¤„ç†åºåˆ—å½¢å¼çš„äº¤é€šç¯å¢ƒç‰¹å¾"""
    
    def __init__(self, num_features: int = 4, hidden_dim: int = 128):
        super().__init__()
        # äº¤é€šç¯å¢ƒç‰¹å¾æ˜¯åºåˆ—å½¢å¼ [batch_size, history_length, num_features]
        # ä½¿ç”¨LSTMå¤„ç†åºåˆ—ç‰¹å¾
        self.lstm = nn.LSTM(num_features, hidden_dim, batch_first=True)
        self.dropout = nn.Dropout(0.2)
        
        # è¾“å‡ºå±‚
        self.out_layer = nn.Linear(hidden_dim, hidden_dim)
        self.relu = nn.ReLU()
    
    def forward(self, x):
        # x: [batch_size, history_length, num_features]
        # ä½¿ç”¨LSTMå¤„ç†åºåˆ—ç‰¹å¾
        lstm_out, _ = self.lstm(x)
        # å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å‡º
        last_out = lstm_out[:, -1, :]
        last_out = self.dropout(last_out)
        
        # é€šè¿‡è¾“å‡ºå±‚
        out = self.out_layer(last_out)
        out = self.relu(out)
        
        return out

class AttentionFusion(nn.Module):
    """æ³¨æ„åŠ›èåˆæ¨¡å—"""
    
    def __init__(self, feature_dim: int = 128):
        super().__init__()
        self.feature_dim = feature_dim
        
        # è‡ªæ³¨æ„åŠ›æœºåˆ¶
        self.query = nn.Linear(feature_dim, feature_dim)
        self.key = nn.Linear(feature_dim, feature_dim)
        self.value = nn.Linear(feature_dim, feature_dim)
        
        # è·¨æ¨¡æ€æ³¨æ„åŠ›
        self.cross_attention = nn.MultiheadAttention(
            embed_dim=feature_dim,
            num_heads=8,
            dropout=0.1
        )
        
        # è¾“å‡ºæŠ•å½±
        self.output_proj = nn.Linear(feature_dim * 3, feature_dim)
    
    def forward(self, visual_feat, motion_feat, traffic_feat):
        # å †å ç‰¹å¾
        features = torch.stack([visual_feat, motion_feat, traffic_feat], dim=1)
        
        # è‡ªæ³¨æ„åŠ›
        Q = self.query(features)
        K = self.key(features)
        V = self.value(features)
        
        attention_weights = torch.softmax(
            torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(self.feature_dim), 
            dim=-1
        )
        attended_features = torch.matmul(attention_weights, V)
        
        # è·¨æ¨¡æ€æ³¨æ„åŠ›
        attended_features = attended_features.transpose(0, 1)
        cross_attended, _ = self.cross_attention(
            attended_features, attended_features, attended_features
        )
        cross_attended = cross_attended.transpose(0, 1)
        
        # èåˆç‰¹å¾
        fused_features = cross_attended.contiguous().view(cross_attended.size(0), -1)
        output = self.output_proj(fused_features)
        
        return output

class IntentClassifier(nn.Module):
    """å·¦è½¬æ„å›¾åˆ†ç±»å™¨"""
    
    def __init__(self, input_dim: int = 128, hidden_dim: int = 64):
        super().__init__()
        self.classifier = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(hidden_dim // 2, 1),
            nn.Sigmoid()
        )
    
    def forward(self, x):
        return self.classifier(x)

class TrajectoryDecoder(nn.Module):
    """è½¨è¿¹é¢„æµ‹è§£ç å™¨ - ä¿®å¤ç‰ˆ"""
    
    def __init__(self, input_dim: int = 129, hidden_dim: int = 128, output_dim: int = 2, seq_len: int = 12):
        super().__init__()
        self.seq_len = seq_len
        self.hidden_dim = hidden_dim
        self.output_dim = output_dim
        
        # LSTMè§£ç å™¨
        self.lstm = nn.LSTM(
            input_size=input_dim,
            hidden_size=hidden_dim,
            num_layers=2,
            batch_first=True,
            dropout=0.2
        )
        
        # è¾“å‡ºå±‚ - é¢„æµ‹è½¨è¿¹ç‚¹
        self.output_layer = nn.Linear(hidden_dim, output_dim)
        
        # è¾“å…¥æŠ•å½±å±‚ - ç”¨äºå°†è½¨è¿¹ç‚¹å’Œæ„å›¾ä¿¡æ¯æ˜ å°„åˆ°LSTMè¾“å…¥ç»´åº¦
        self.input_proj = nn.Linear(output_dim + 1, input_dim)  # è½¨è¿¹ç‚¹ + æ„å›¾æ¦‚ç‡ -> æ˜ å°„åˆ°LSTMè¾“å…¥ç»´åº¦
        
        # åˆå§‹éšè—çŠ¶æ€
        self.init_hidden = nn.Linear(input_dim, hidden_dim * 2 * 2)  # 2 layers * 2 (h,c)
    
    def forward(self, fused_features, intent_prob):
        batch_size = fused_features.size(0)
        
        # ç»“åˆæ„å›¾ä¿¡æ¯
        initial_input = torch.cat([fused_features, intent_prob], dim=1)  # [batch, input_dim]
        
        # åˆå§‹åŒ–éšè—çŠ¶æ€
        init_states = self.init_hidden(initial_input)
        h0 = init_states[:, :self.hidden_dim*2].reshape(2, batch_size, self.hidden_dim)
        c0 = init_states[:, self.hidden_dim*2:].reshape(2, batch_size, self.hidden_dim)
        
        # è§£ç é¢„æµ‹è½¨è¿¹
        outputs = []
        hidden = (h0, c0)
        
        # ç¬¬ä¸€æ­¥è¾“å…¥
        decoder_input = initial_input.unsqueeze(1)  # [batch, 1, input_dim]
        
        # å¾ªç¯é¢„æµ‹æ¯ä¸€æ­¥è½¨è¿¹
        for t in range(self.seq_len):
            # LSTMå‰å‘ä¼ æ’­
            lstm_output, hidden = self.lstm(decoder_input, hidden)
            # é¢„æµ‹è½¨è¿¹ç‚¹
            trajectory_point = self.output_layer(lstm_output)
            outputs.append(trajectory_point)
            
            # ä¸ºä¸‹ä¸€æ­¥å‡†å¤‡è¾“å…¥ - ä½¿ç”¨ä¸Šä¸€æ­¥çš„è¾“å‡ºä½œä¸ºè¾“å…¥çš„ä¸€éƒ¨åˆ†
            # å°†æ„å›¾æ¦‚ç‡æ‰©å±•åˆ°ä¸è½¨è¿¹ç‚¹ç›¸åŒçš„ç»´åº¦
            intent_prob_expanded = intent_prob.unsqueeze(1).expand(-1, 1, -1)
            # ç»„åˆè½¨è¿¹ç‚¹å’Œæ„å›¾ä¿¡æ¯
            next_input_part = torch.cat([trajectory_point, intent_prob_expanded], dim=2)
            # ä½¿ç”¨åŸå§‹èåˆç‰¹å¾ä½œä¸ºåŸºç¡€ï¼Œç¡®ä¿æ¯ä¸€æ­¥éƒ½èƒ½è·å–åˆ°åˆå§‹ç‰¹å¾ä¿¡æ¯
            next_input = decoder_input + self.input_proj(next_input_part)
            decoder_input = next_input
        
        # æ‹¼æ¥æ‰€æœ‰è¾“å‡º
        trajectory = torch.cat(outputs, dim=1)
        
        return trajectory

class LeftTurnPredictor(nn.Module):
    """
    å·¦è½¬é¢„æµ‹æ¨¡å‹ï¼Œé‡‡ç”¨å¤šæ¨¡æ€èåˆæ¶æ„
    æ•´åˆäº†è§†è§‰ã€è¿åŠ¨å’Œäº¤é€šç¯å¢ƒç‰¹å¾è¿›è¡Œå·¦è½¬æ„å›¾é¢„æµ‹å’Œè½¨è¿¹é¢„æµ‹
    """

    def __init__(self, history_horizon: int = 50, num_motion_features: int = 6, 
                 num_traffic_features: int = 4, num_visual_features: int = 32, prediction_horizon: int = 12):
        super().__init__()
        
        # è¿åŠ¨å­¦ç‰¹å¾ç¼–ç å™¨ï¼ˆä½¿ç”¨LSTMï¼‰
        self.motion_encoder = MotionEncoder(num_features=num_motion_features)
        
        # äº¤é€šç¯å¢ƒç‰¹å¾ç¼–ç å™¨
        self.traffic_encoder = TrafficEncoder(num_features=num_traffic_features)
        
        # è§†è§‰ç‰¹å¾ç¼–ç å™¨
        self.visual_encoder = VisualEncoder(input_dim=num_visual_features)

        # æ³¨æ„åŠ›èåˆæ¨¡å—
        self.attention_fusion = AttentionFusion()

        # æ„å›¾åˆ†ç±»å™¨
        self.intent_classifier = IntentClassifier()

        # è½¨è¿¹è§£ç å™¨ - ä½¿ç”¨åŠ¨æ€é¢„æµ‹é•¿åº¦
        self.trajectory_decoder = TrajectoryDecoder(seq_len=prediction_horizon)

    def forward(self, visual_feat, motion_feat, traffic_feat):
        # ç‰¹å¾ç¼–ç 
        # æ‰€æœ‰ç‰¹å¾éƒ½æ˜¯åºåˆ—å½¢å¼ [batch_size, history_length, feature_dim]
        visual_encoded = self.visual_encoder(visual_feat)
        motion_encoded = self.motion_encoder(motion_feat)
        traffic_encoded = self.traffic_encoder(traffic_feat)
        
        # å¤šæ¨¡æ€èåˆ
        fused_features = self.attention_fusion(
            visual_encoded, motion_encoded, traffic_encoded
        )
        
        # æ„å›¾é¢„æµ‹
        intent_prob = self.intent_classifier(fused_features)
        
        # è½¨è¿¹é¢„æµ‹
        trajectory = self.trajectory_decoder(fused_features, intent_prob)
        
        return intent_prob, trajectory

class TrainingManager:
    """è®­ç»ƒç®¡ç†å™¨"""
    
    def __init__(self, model, train_loader, val_loader, device='cuda'):
        self.model = model.to(device)
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.device = device
        
        # ä¼˜åŒ–å™¨
        self.optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer, mode='min', patience=10, factor=0.5
        )
        
        # æŸå¤±å‡½æ•°
        # é’ˆå¯¹ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ï¼Œä½¿ç”¨å¸¦æƒé‡çš„äº¤å‰ç†µæŸå¤±
        # ç”±äºè®­ç»ƒé›†ä¸­å·¦è½¬æ ·æœ¬å æ¯”çº¦0.9%ï¼Œä½¿ç”¨è¾ƒé«˜çš„æƒé‡
        self.intent_loss_fn = nn.BCELoss(reduction='none')
        self.trajectory_loss_fn = nn.MSELoss()
        self.left_turn_weight = 10.0  # å·¦è½¬æ ·æœ¬çš„æƒé‡ï¼Œæ ¹æ®æ•°æ®åˆ†å¸ƒè°ƒæ•´
        
        # è®­ç»ƒå†å²
        self.train_history = {'loss': [], 'intent_acc': [], 'traj_error': []}
        self.val_history = {'loss': [], 'intent_acc': [], 'traj_error': []}
    
    def train_epoch(self, epoch_num=None, total_epochs=None):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0
        intent_correct = 0
        total_samples = 0
        traj_error = 0
        
        # åˆ›å»ºè¿›åº¦æ¡
        desc = f"Epoch {epoch_num}/{total_epochs} [Train]" if epoch_num and total_epochs else "Training"
        pbar = tqdm(self.train_loader, desc=desc, leave=False)
        
        batch_losses = []
        batch_start_time = time.time()
        
        for batch_idx, batch in enumerate(pbar):
            # æ•°æ®ç§»åˆ°è®¾å¤‡
            visual_feat = batch['visual_features'].to(self.device)
            motion_feat = batch['motion_features'].to(self.device)
            traffic_feat = batch['traffic_features'].to(self.device)
            intent_target = batch['left_turn_intent'].to(self.device)
            traj_target = batch['target_trajectory'].to(self.device)
            
            # å‰å‘ä¼ æ’­
            intent_pred, traj_pred = self.model(visual_feat, motion_feat, traffic_feat)
            
            # è®¡ç®—æŸå¤±
            # åº”ç”¨ç±»åˆ«æƒé‡è§£å†³ä¸å¹³è¡¡é—®é¢˜
            # å¯¹å·¦è½¬æ ·æœ¬(intent_target > 0.5)åº”ç”¨è¾ƒé«˜æƒé‡
            weights = torch.where(intent_target > 0.5, self.left_turn_weight, 1.0)
            intent_loss = (self.intent_loss_fn(intent_pred, intent_target) * weights).mean()
            traj_loss = self.trajectory_loss_fn(traj_pred, traj_target)
            
            # è”åˆæŸå¤± - è°ƒæ•´æƒé‡æ¯”ä¾‹ï¼Œå‡å°‘è½¨è¿¹é¢„æµ‹æŸå¤±çš„å½±å“
            total_batch_loss = intent_loss + 0.1 * traj_loss
            
            # åå‘ä¼ æ’­
            self.optimizer.zero_grad()
            total_batch_loss.backward()
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
            self.optimizer.step()
            
            # ç»Ÿè®¡
            batch_loss = total_batch_loss.item()
            total_loss += batch_loss
            batch_losses.append(batch_loss)
            intent_correct += ((intent_pred > 0.5) == (intent_target > 0.5)).sum().item()
            total_samples += intent_target.size(0)
            traj_error += torch.sqrt(torch.mean((traj_pred - traj_target) ** 2)).item()
            
            # æ›´æ–°è¿›åº¦æ¡ä¿¡æ¯
            if batch_idx % 5 == 0:  # æ¯5ä¸ªbatchæ›´æ–°ä¸€æ¬¡
                current_avg_loss = np.mean(batch_losses[-10:]) if len(batch_losses) >= 10 else np.mean(batch_losses)
                current_intent_acc = intent_correct / total_samples if total_samples > 0 else 0
                
                # è®¡ç®—å¤„ç†é€Ÿåº¦
                elapsed_time = time.time() - batch_start_time
                samples_per_sec = total_samples / elapsed_time if elapsed_time > 0 else 0
                
                pbar.set_postfix({
                    'Loss': f'{current_avg_loss:.4f}',
                    'IntentAcc': f'{current_intent_acc:.3f}',
                    'Speed': f'{samples_per_sec:.1f}samples/s'
                })
        
        pbar.close()
        
        avg_loss = total_loss / len(self.train_loader)
        intent_acc = intent_correct / total_samples
        avg_traj_error = traj_error / len(self.train_loader)
        
        return avg_loss, intent_acc, avg_traj_error
    
    def validate(self, epoch_num=None, total_epochs=None):
        """éªŒè¯"""
        self.model.eval()
        total_loss = 0
        intent_correct = 0
        total_samples = 0
        traj_error = 0
        
        # åˆ›å»ºéªŒè¯è¿›åº¦æ¡
        desc = f"Epoch {epoch_num}/{total_epochs} [Valid]" if epoch_num and total_epochs else "Validating"
        pbar = tqdm(self.val_loader, desc=desc, leave=False)
        
        with torch.no_grad():
            for batch_idx, batch in enumerate(pbar):
                # æ•°æ®ç§»åˆ°è®¾å¤‡
                visual_feat = batch['visual_features'].to(self.device)
                motion_feat = batch['motion_features'].to(self.device)
                traffic_feat = batch['traffic_features'].to(self.device)
                intent_target = batch['left_turn_intent'].to(self.device)
                traj_target = batch['target_trajectory'].to(self.device)
                
                # å‰å‘ä¼ æ’­
                intent_pred, traj_pred = self.model(visual_feat, motion_feat, traffic_feat)
                
                # è®¡ç®—æŸå¤±
                # åº”ç”¨ç±»åˆ«æƒé‡è§£å†³ä¸å¹³è¡¡é—®é¢˜
                # å¯¹å·¦è½¬æ ·æœ¬(intent_target > 0.5)åº”ç”¨è¾ƒé«˜æƒé‡
                weights = torch.where(intent_target > 0.5, self.left_turn_weight, 1.0)
                intent_loss = (self.intent_loss_fn(intent_pred, intent_target) * weights).mean()
                traj_loss = self.trajectory_loss_fn(traj_pred, traj_target)
                
                # è”åˆæŸå¤± - ä¸è®­ç»ƒä¿æŒä¸€è‡´çš„æƒé‡æ¯”ä¾‹
                total_batch_loss = intent_loss + 0.1 * traj_loss
                
                # ç»Ÿè®¡
                total_loss += total_batch_loss.item()
                intent_correct += ((intent_pred > 0.5) == (intent_target > 0.5)).sum().item()
                total_samples += intent_target.size(0)
                traj_error += torch.sqrt(torch.mean((traj_pred - traj_target) ** 2)).item()
                
                # æ›´æ–°è¿›åº¦æ¡ä¿¡æ¯
                if batch_idx % 3 == 0:  # éªŒè¯æ—¶æ›´é¢‘ç¹æ›´æ–°
                    current_avg_loss = total_loss / (batch_idx + 1)
                    current_intent_acc = intent_correct / total_samples if total_samples > 0 else 0
                    
                    pbar.set_postfix({
                        'Loss': f'{current_avg_loss:.4f}',
                        'IntentAcc': f'{current_intent_acc:.3f}'
                    })
        
        pbar.close()
        
        avg_loss = total_loss / len(self.val_loader)
        intent_acc = intent_correct / total_samples
        avg_traj_error = traj_error / len(self.val_loader)
        
        return avg_loss, intent_acc, avg_traj_error
    
    def train(self, epochs: int = 50, early_stopping_patience: int = 15):
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        best_val_loss = float('inf')
        patience_counter = 0
        
        print("ğŸš€ å¼€å§‹è®­ç»ƒ...")
        print(f"ğŸ“Š è®­ç»ƒé›†å¤§å°: {len(self.train_loader.dataset):,}")
        print(f"ğŸ“Š éªŒè¯é›†å¤§å°: {len(self.val_loader.dataset):,}")
        print(f"ğŸ¯ ç›®æ ‡è½®æ•°: {epochs}")
        print(f"â° æ—©åœè€å¿ƒ: {early_stopping_patience}")
        print(f"ğŸ’» è®¾å¤‡: {self.device}")
        print("=" * 80)
        
        # æ€»ä½“è¿›åº¦æ¡
        epoch_pbar = tqdm(range(epochs), desc="Overall Progress", position=0)
        
        training_start_time = time.time()
        
        for epoch in epoch_pbar:
            epoch_start_time = time.time()
            
            # è®­ç»ƒ
            train_loss, train_intent_acc, train_traj_error = self.train_epoch(epoch+1, epochs)
            
            # éªŒè¯
            val_loss, val_intent_acc, val_traj_error = self.validate(epoch+1, epochs)
            
            # å­¦ä¹ ç‡è°ƒåº¦
            old_lr = self.optimizer.param_groups[0]['lr']
            self.scheduler.step(val_loss)
            new_lr = self.optimizer.param_groups[0]['lr']
            
            # è®°å½•å†å²
            self.train_history['loss'].append(train_loss)
            self.train_history['intent_acc'].append(train_intent_acc)
            self.train_history['traj_error'].append(train_traj_error)
            
            self.val_history['loss'].append(val_loss)
            self.val_history['intent_acc'].append(val_intent_acc)
            self.val_history['traj_error'].append(val_traj_error)
            
            # è®¡ç®—æ—¶é—´ç»Ÿè®¡
            epoch_time = time.time() - epoch_start_time
            total_elapsed = time.time() - training_start_time
            avg_epoch_time = total_elapsed / (epoch + 1)
            eta = avg_epoch_time * (epochs - epoch - 1)
            
            # æ—©åœæ£€æŸ¥å’Œæ¨¡å‹ä¿å­˜
            improvement = ""
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                patience_counter = 0
                torch.save(self.model.state_dict(), 'best_model.pth')
                improvement = "ğŸ’¾ [BEST]"
            else:
                patience_counter += 1
                if patience_counter >= early_stopping_patience:
                    improvement = "â¹ï¸ [EARLY STOP]"
            
            # å­¦ä¹ ç‡å˜åŒ–æç¤º
            lr_info = f"ğŸ“‰ LR: {old_lr:.2e}" if new_lr == old_lr else f"ğŸ“‰ LR: {old_lr:.2e}â†’{new_lr:.2e}"
            
            # æ›´æ–°æ€»ä½“è¿›åº¦æ¡
            epoch_pbar.set_postfix({
                'Train_Loss': f'{train_loss:.4f}',
                'Val_Loss': f'{val_loss:.4f}',
                'Val_Acc': f'{val_intent_acc:.3f}',
                'Patience': f'{patience_counter}/{early_stopping_patience}',
                'ETA': f'{eta/60:.1f}min'
            })
            
            # è¯¦ç»†ä¿¡æ¯è¾“å‡º
            epoch_info = f"ğŸ“ˆ Epoch {epoch+1:3d}/{epochs} | " + \
                        f"â±ï¸ {epoch_time:.1f}s | " + \
                        f"ğŸ”„ Train: {train_loss:.4f} | " + \
                        f"âœ… Valid: {val_loss:.4f} | " + \
                        f"ğŸ¯ Acc: {val_intent_acc:.3f} | " + \
                        f"ğŸ“ TrajErr: {val_traj_error:.3f}"
            print(epoch_info)
            
            detail_info = f"    {lr_info} | " + \
                         f"â³ ETA: {eta/60:.1f}min | " + \
                         f"ğŸ• Total: {total_elapsed/60:.1f}min | " + \
                         f"{improvement}"
            print(detail_info)
            
            if patience_counter >= early_stopping_patience:
                print(f"â¹ï¸ æ—©åœè§¦å‘ï¼åœ¨ç¬¬ {epoch+1} è½®åœæ­¢è®­ç»ƒ")
                print(f"ğŸ† æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.4f}")
                break
        
        epoch_pbar.close()
        
        total_training_time = time.time() - training_start_time
        print("" + "=" * 80)
        print("ğŸ‰ è®­ç»ƒå®Œæˆï¼")
        print(f"â±ï¸ æ€»è®­ç»ƒæ—¶é—´: {total_training_time/60:.1f} åˆ†é’Ÿ")
        print(f"ğŸ† æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.4f}")
        print(f"ğŸ’¾ æœ€ä½³æ¨¡å‹å·²ä¿å­˜ä¸º: best_model.pth")
        print("=" * 80)
        
        return self.train_history, self.val_history
    
    def plot_training_history(self):
        """ç»˜åˆ¶è®­ç»ƒå†å²"""
        fig, axes = plt.subplots(1, 3, figsize=(15, 5))
        
        # æŸå¤±æ›²çº¿
        axes[0].plot(self.train_history['loss'], label='Train Loss')
        axes[0].plot(self.val_history['loss'], label='Val Loss')
        axes[0].set_title('Training Loss')
        axes[0].set_xlabel('Epoch')
        axes[0].set_ylabel('Loss')
        axes[0].legend()
        axes[0].grid(True)
        
        # æ„å›¾è¯†åˆ«å‡†ç¡®ç‡
        axes[1].plot(self.train_history['intent_acc'], label='Train Acc')
        axes[1].plot(self.val_history['intent_acc'], label='Val Acc')
        axes[1].set_title('Intent Classification Accuracy')
        axes[1].set_xlabel('Epoch')
        axes[1].set_ylabel('Accuracy')
        axes[1].legend()
        axes[1].grid(True)
        
        # è½¨è¿¹é¢„æµ‹è¯¯å·®
        axes[2].plot(self.train_history['traj_error'], label='Train Error')
        axes[2].plot(self.val_history['traj_error'], label='Val Error')
        axes[2].set_title('Trajectory Prediction Error')
        axes[2].set_xlabel('Epoch')
        axes[2].set_ylabel('RMSE')
        axes[2].legend()
        axes[2].grid(True)
        
        plt.tight_layout()
        plt.savefig('training_history.png', dpi=300, bbox_inches='tight')
        plt.show()

def evaluate_model(model, test_loader, device='cuda'):
    """æ¨¡å‹è¯„ä¼°"""
    model.eval()
    
    all_intent_preds = []
    all_intent_targets = []
    all_traj_preds = []
    all_traj_targets = []
    all_future_movements = []  # å­˜å‚¨future_movementsç”¨äºä¸çœŸå®å·¦è½¬æ•°æ®éªŒè¯
    
    print("ğŸ“ˆ å¼€å§‹è¯„ä¼°æ¨¡å‹æ€§èƒ½...")
    
    # ä½¿ç”¨è¿›åº¦æ¡æ˜¾ç¤ºè¯„ä¼°è¿›åº¦
    with torch.no_grad():
        with tqdm(total=len(test_loader), desc="è¯„ä¼°è¿›åº¦", unit="æ‰¹") as pbar:
            for batch_idx, batch in enumerate(test_loader):
                visual_feat = batch['visual_features'].to(device)
                motion_feat = batch['motion_features'].to(device)
                traffic_feat = batch['traffic_features'].to(device)
                intent_target = batch['left_turn_intent'].to(device)
                traj_target = batch['target_trajectory'].to(device)
                
                # æ£€æŸ¥æ˜¯å¦åŒ…å«future_movementså­—æ®µï¼ˆç”¨äºä¸çœŸå®å·¦è½¬æ•°æ®éªŒè¯ï¼‰
                if 'future_movements' in batch:
                    all_future_movements.extend(batch['future_movements'])
                
                intent_pred, traj_pred = model(visual_feat, motion_feat, traffic_feat)
                
                all_intent_preds.append(intent_pred.cpu().numpy())
                all_intent_targets.append(intent_target.cpu().numpy())
                all_traj_preds.append(traj_pred.cpu().numpy())
                all_traj_targets.append(traj_target.cpu().numpy())
                
                # æ›´æ–°è¿›åº¦æ¡
                pbar.update(1)
                
                # å®šæœŸæ˜¾ç¤ºå½“å‰è¿›åº¦çš„ç»Ÿè®¡ä¿¡æ¯
                if batch_idx % 10 == 0 or batch_idx == len(test_loader) - 1:
                    # è®¡ç®—å½“å‰çš„ç®€å•ç»Ÿè®¡
                    current_samples = (batch_idx + 1) * len(batch['visual_features'])
                    pbar.set_postfix({
                        'å·²å¤„ç†æ ·æœ¬': current_samples
                    })
    
    print("âœ… æ¨¡å‹è¯„ä¼°å®Œæˆï¼Œå¼€å§‹è®¡ç®—æ€§èƒ½æŒ‡æ ‡...")
    
    # åˆå¹¶ç»“æœ
    intent_preds = np.concatenate(all_intent_preds)
    intent_targets = np.concatenate(all_intent_targets)
    traj_preds = np.concatenate(all_traj_preds)
    traj_targets = np.concatenate(all_traj_targets)
    
    # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
    
    # æ„å›¾è¯†åˆ«æŒ‡æ ‡
    intent_binary_preds = (intent_preds > 0.5).astype(int)
    intent_binary_targets = (intent_targets > 0.5).astype(int)
    
    intent_accuracy = accuracy_score(intent_binary_targets, intent_binary_preds)
    intent_precision = precision_score(intent_binary_targets, intent_binary_preds)
    intent_recall = recall_score(intent_binary_targets, intent_binary_preds)
    intent_f1 = f1_score(intent_binary_targets, intent_binary_preds)
    
    # è½¨è¿¹é¢„æµ‹æŒ‡æ ‡
    # ADE (Average Displacement Error)
    ade = np.mean(np.sqrt(np.sum((traj_preds - traj_targets) ** 2, axis=2)))
    
    # FDE (Final Displacement Error)
    fde = np.mean(np.sqrt(np.sum((traj_preds[:, -1, :] - traj_targets[:, -1, :]) ** 2, axis=1)))
    
    # æ‰“å°è¯„ä¼°ç»“æœ
    print("============================================================")
    print("                        æ¨¡å‹è¯„ä¼°ç»“æœ")
    print("============================================================")
    print(f"æ„å›¾è¯†åˆ«å‡†ç¡®ç‡: {intent_accuracy:.4f}")
    print(f"æ„å›¾è¯†åˆ«ç²¾ç¡®ç‡: {intent_precision:.4f}")
    print(f"æ„å›¾è¯†åˆ«å¬å›ç‡: {intent_recall:.4f}")
    print(f"æ„å›¾è¯†åˆ«F1åˆ†æ•°: {intent_f1:.4f}")
    print("-" * 40)
    print(f"è½¨è¿¹é¢„æµ‹ADE: {ade:.4f} m")
    print(f"è½¨è¿¹é¢„æµ‹FDE: {fde:.4f} m")
    print("=" * 60)
    
    # å‡†å¤‡è¿”å›ç»“æœ
    results = {
        'intent_accuracy': intent_accuracy,
        'intent_precision': intent_precision,
        'intent_recall': intent_recall,
        'intent_f1': intent_f1,
        'trajectory_ade': ade,
        'trajectory_fde': fde
    }
    
    # å¦‚æœå­˜åœ¨future_movementsæ•°æ®ï¼Œè¿›è¡Œä¸çœŸå®å·¦è½¬æ•°æ®çš„éªŒè¯åˆ†æ
    if all_future_movements:
        # åŸºäºfuture_movementsç¡®å®šçœŸæ­£çš„å·¦è½¬è½¦è¾†
        # ä¿®å¤ï¼šæ ¹æ®ä»£ç ä¸­ä½¿ç”¨çš„æ•°å€¼è¡¨ç¤ºï¼ˆ2.0 = å·¦è½¬ï¼‰æ¥åˆ¤æ–­
        true_left_turns = [1 if any(m == 2.0 for m in movements) else 0 for movements in all_future_movements]
        
        # è®¡ç®—åŸºäºçœŸå®å·¦è½¬æ ‡ç­¾çš„å‡†ç¡®ç‡
        true_left_accuracy = accuracy_score(true_left_turns, intent_binary_preds[:len(true_left_turns)])
        true_left_precision = precision_score(true_left_turns, intent_binary_preds[:len(true_left_turns)], zero_division=0)
        true_left_recall = recall_score(true_left_turns, intent_binary_preds[:len(true_left_turns)], zero_division=0)
        true_left_f1 = f1_score(true_left_turns, intent_binary_preds[:len(true_left_turns)], zero_division=0)
        
        # æ‰“å°åŸºäºçœŸå®å·¦è½¬æ•°æ®çš„éªŒè¯ç»“æœ
        print("ğŸ“Š åŸºäºçœŸå®å·¦è½¬æ•°æ®çš„éªŒè¯ç»“æœ (ä¸å·¦è½¬æ•°æ®åˆ†æè„šæœ¬å¯¹æ¯”):")
        print(f"   - çœŸæ­£å·¦è½¬è½¦è¾†æ•°: {sum(true_left_turns)}/{len(true_left_turns)}")
        print(f"   - é¢„æµ‹å‡†ç¡®ç‡: {true_left_accuracy:.4f}")
        print(f"   - ç²¾ç¡®ç‡: {true_left_precision:.4f}")
        print(f"   - å¬å›ç‡: {true_left_recall:.4f}")
        print(f"   - F1åˆ†æ•°: {true_left_f1:.4f}")
        print("=" * 60)
        
        # å°†éªŒè¯ç»“æœæ·»åŠ åˆ°è¿”å›å­—å…¸
        results.update({
            'true_left_turns_count': sum(true_left_turns),
            'true_left_turns_total': len(true_left_turns),
            'true_left_accuracy': true_left_accuracy,
            'true_left_precision': true_left_precision,
            'true_left_recall': true_left_recall,
            'true_left_f1': true_left_f1
        })
    
    return results

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ è½¦è¾†å·¦è½¬è½¨è¿¹é¢„æµ‹ç³»ç»Ÿ")
    print("åŸºäºå†å²è½¨è¿¹çš„çœŸæ­£å·¦è½¬æ„å›¾é¢„æµ‹")
    print("=" * 60)
    data_dir = os.path.join(os.path.dirname(__file__), "../data")

    # åŠ è½½é…ç½® (ä½¿ç”¨é»˜è®¤è·¯å¾„ï¼Œè‡ªåŠ¨å®šä½åˆ°è„šæœ¬æ‰€åœ¨ç›®å½•çš„config.yaml)
    config = load_config()

    raw_csv_file =  config.get("raw_csv_file", "peachtree_filtered_data.csv")
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ’» ä½¿ç”¨è®¾å¤‡: {device}")
    
    # æ·»åŠ å…¨å±€æ ·æœ¬æ•°é™åˆ¶å‚æ•°
    max_samples_input = input("è¯·è¾“å…¥è¦å¤„ç†çš„æœ€å¤§æ ·æœ¬æ•° (é»˜è®¤: å…¨éƒ¨ï¼Œè¾“å…¥æ­£æ•´æ•°å¯ç¼©å‡è°ƒè¯•æ—¶é—´): ").strip()

    max_samples = config.get("max_samples", -1)
    max_samples = int(max_samples_input) if max_samples_input and max_samples_input.isdigit() and int(max_samples_input)>0 else max_samples
    
    print("âœ… ä½¿ç”¨çœŸæ­£çš„é¢„æµ‹æ¨¡å¼")
    print("   - å†å²é•¿åº¦: 30å¸§ (3ç§’)")
    print("   - é¢„æµ‹èŒƒå›´: 50å¸§ (5ç§’)")
    print("   - åˆ©ç”¨NGSIM movementæ ‡ç­¾è¿›è¡ŒçœŸæ­£çš„é¢„æµ‹")
    if max_samples>0:
        print(f"   - é™åˆ¶æœ€å¤§æ ·æœ¬æ•°: {max_samples}")
    
    # åˆ›å»ºæ•°æ®é›†
    print("ğŸ”„ åˆ›å»ºæ•°æ®é›†...")
    try:
        # è·å–æ•°æ®è·¯å¾„
        raw_csv_file_fullpath = f"{data_dir}/{raw_csv_file}"
        data_path_input = input(f"è¯·è¾“å…¥NGSIMæ•°æ®è·¯å¾„ (é»˜è®¤: {raw_csv_file_fullpath}): ").strip()
        data_path_fullpath = data_path_input if data_path_input else raw_csv_file_fullpath
        
        # æ£€æŸ¥æ•°æ®æ˜¯å¦å­˜åœ¨
        if not os.path.exists(data_path_fullpath):
            print(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_path_fullpath}")
            print("ğŸ’¡ è¯·ç¡®ä¿NGSIMæ•°æ®æ–‡ä»¶å­˜åœ¨")
            return
            
        # åˆ›å»ºDataPipelineå®ä¾‹
        data_pipeline = DataPipeline(data_path_fullpath)
        
        # è·å–è·¯å£å’Œæ–¹å‘ä¿¡æ¯
        int_id = config.get("int_id", 1)
        approach = config.get("approach", "northbound")
        
        # é¢„æµ‹æ¨¡å¼ä¸‹å¯ä»¥é€‰æ‹©ç‰¹å®šè·¯å£å’Œæ–¹å‘
        filter_input = input("æ˜¯å¦æŒ‰è·¯å£å’Œæ–¹å‘ç­›é€‰æ•°æ®ï¼Ÿ(y/n, é»˜è®¤: y): ").strip().lower()
        filter_input = 'y' if not filter_input else filter_input
        if filter_input == 'y':
            try:
                # å…ˆä½¿ç”¨LeftTurnAnalyzerå‘ç°å¹¶æ˜¾ç¤ºæ•°æ®ä¸­çš„è·¯å£
                print("ğŸ” æ­£åœ¨åˆ†ææ•°æ®ä¸­çš„è·¯å£ä¿¡æ¯...")
                analyzer = LeftTurnAnalyzer(data_path_fullpath)
                analyzer.load_data()
                intersections = analyzer.discover_intersections()
                
                if intersections:
                    print("\nğŸ“‹ æ•°æ®ä¸­å‘ç°çš„è·¯å£ä¿¡æ¯ï¼š")
                    print("=" * 80)
                    print(f"{'è·¯å£ID':<8} {'æ€»è®°å½•æ•°':<12} {'è½¦è¾†æ•°':<10} {'æ–¹å‘':<20} {'æœºåŠ¨ç±»å‹':<15}")
                    print("-" * 80)
                    
                    for int_id_available, info in sorted(intersections.items()):
                        # è½¬æ¢æ–¹å‘ä¸ºåç§°
                        direction_names = []
                        for direction in info['directions'][:4]:  # æœ€å¤šæ˜¾ç¤º4ä¸ªæ–¹å‘
                            if direction in analyzer.direction_names:
                                direction_names.append(f"{direction}({analyzer.direction_names[direction].split(' ')[0]})")
                            else:
                                direction_names.append(str(direction))
                        directions_str = ','.join(direction_names)
                        
                        # è½¬æ¢æœºåŠ¨ç±»å‹ä¸ºåç§°
                        movement_names = []
                        for movement in info['movements'][:4]:  # æœ€å¤šæ˜¾ç¤º4ä¸ªæœºåŠ¨ç±»å‹
                            if movement in analyzer.movement_names:
                                movement_names.append(f"{movement}({analyzer.movement_names[movement].split(' ')[0]})")
                            else:
                                movement_names.append(str(movement))
                        movements_str = ','.join(movement_names)
                        
                        print(f"{int_id_available:<8} {info['total_records']:<12} {info['total_vehicles']:<10} {directions_str:<20} {movements_str:<15}")
                    print("=" * 80)
                else:
                    print("âš ï¸ æœªèƒ½åœ¨æ•°æ®ä¸­å‘ç°è·¯å£ä¿¡æ¯")
                
                # ç„¶åè®©ç”¨æˆ·é€‰æ‹©è·¯å£ID
                int_id_input = input("è¯·è¾“å…¥è·¯å£ID (ç•™ç©ºä¸ç­›é€‰): ").strip()
                int_id = int(int_id_input) if int_id_input else None
                
                if int_id is not None:
                    # åˆ†æè¯¥è·¯å£çš„å¯ç”¨å…¥å£æ–¹å‘
                    try:
                        entrance_analyzer = LeftTurnAnalyzer(data_path_fullpath)
                        entrance_analyzer.load_data()
                        entrance_analyzer.intersection_id = int_id
                        entrance_stats = entrance_analyzer.analyze_intersection_entrances()
                        
                        if entrance_stats:
                            print(f"\nâœ… è·¯å£ {int_id} çš„å¯ç”¨å…¥å£æ–¹å‘ä¿¡æ¯ï¼š")
                            print("=" * 70)
                            print(f"{'æ–¹å‘ç¼–å·':<10} {'æ–¹å‘åç§°':<10} {'æ€»è½¦è¾†':<10} {'å·¦è½¬è½¦è¾†':<10} {'å·¦è½¬æ¯”ä¾‹':<10}")
                            print("-" * 70)
                            
                            for stats in entrance_stats.values():
                                print(f"{stats['direction']:<10} {stats['direction_name']:<10} {stats['total_vehicles']:<10} {stats['left_turn_vehicles']:<10} {stats['left_turn_ratio']:.1f}%")
                            print("=" * 70)
                    except Exception as e:
                        print(f"âš ï¸ åˆ†æå…¥å£æ–¹å‘æ—¶å‡ºé”™: {e}")
                    
                    approach_input = input("è¯·è¾“å…¥å…¥å£æ–¹å‘ (1-ä¸œ, 2-åŒ—, 3-è¥¿, 4-å—, ç•™ç©ºä¸ç­›é€‰): ").strip()
                    approach = int(approach_input) if approach_input and approach_input.isdigit() else None
            except ValueError:
                print("âš ï¸ æ— æ•ˆçš„è·¯å£IDæˆ–æ–¹å‘ï¼Œå°†ä½¿ç”¨æ‰€æœ‰æ•°æ®")
        
        # ä½¿ç”¨DataPipelineæ„å»ºæ•°æ®é›†
        history_length = config.get("history_length", 30)
        prediction_horizon = config.get("prediction_horizon", 12)  # ä¿®æ”¹ä¸º12ä»¥åŒ¹é…æ¨¡å‹è¾“å‡º

        epochs = config.get("epochs", 50)
        epochs_input = input("è¯·è¾“å…¥è®­ç»ƒè½®æ•° epochs (é»˜è®¤: epochs=50): ").strip()
        epochs = int(epochs_input) if epochs_input else epochs

        # å¦‚æœç”¨æˆ·é€‰æ‹©ä¸æŒ‰è·¯å£å’Œæ–¹å‘ç­›é€‰ï¼Œåˆ™å°†int_idå’Œapproachè®¾ä¸ºNone
        if filter_input == 'n':
            print("âœ… ç”¨æˆ·é€‰æ‹©ä¸æŒ‰è·¯å£å’Œæ–¹å‘ç­›é€‰æ•°æ®ï¼Œä½¿ç”¨å…¨éƒ¨æ•°æ®")
            build_int_id = None
            build_approach = None
        else:
            build_int_id = int_id
            build_approach = approach

        full_dataset = data_pipeline.build_dataset(
            int_id=build_int_id,
            approach=build_approach,
            history_length=history_length,
            prediction_horizon=prediction_horizon,
            min_trajectory_length=100,
            max_samples=max_samples
        )
        
        # åˆ†ææ•°æ®é›†
        data_pipeline.get_dataset_statistics(full_dataset)
        
        # ä½¿ç”¨DataPipelineè¿›è¡Œæ•°æ®é›†åˆ’åˆ†
        train_dataset, val_dataset, test_dataset = data_pipeline.split_dataset(full_dataset)
        
        # åˆ†æå„æ•°æ®é›†çš„å·¦è½¬è½¦è¾†åˆ†å¸ƒ
        data_pipeline.analyze_dataset_split(train_dataset, val_dataset, test_dataset)
        
    except Exception as e:
        print(f"âŒ æ•°æ®å¤„ç†å¤±è´¥: {e}")
        print("ğŸ”„ ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®è¿›è¡Œæ¼”ç¤º...")
        # ä½¿ç”¨ä¸é…ç½®ä¸€è‡´çš„history_length
        train_dataset = MockDataset(800, history_length=history_length)
        val_dataset = MockDataset(200, history_length=history_length)
        test_dataset = MockDataset(200, history_length=history_length)
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)
    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)
    
    # åˆ›å»ºæ¨¡å‹ - ä¼ å…¥é¢„æµ‹é•¿åº¦å‚æ•°
    print("åˆ›å»ºæ¨¡å‹...")
    model = LeftTurnPredictor(prediction_horizon=prediction_horizon)
    
    # åˆ›å»ºè®­ç»ƒç®¡ç†å™¨
    trainer = TrainingManager(model, train_loader, val_loader, device)
    
    # è®­ç»ƒæ¨¡å‹
    print("å¼€å§‹è®­ç»ƒ...")
    train_history, val_history = trainer.train(epochs=epochs)
    
    # ç»˜åˆ¶è®­ç»ƒå†å²
    trainer.plot_training_history()
    
    # åŠ è½½æœ€ä½³æ¨¡å‹
    model.load_state_dict(torch.load('best_model.pth'))
    
    # è¯„ä¼°æ¨¡å‹
    print("è¯„ä¼°æ¨¡å‹...")
    results = evaluate_model(model, test_loader, device)
    
    print("è®­ç»ƒå’Œè¯„ä¼°å®Œæˆï¼")

if __name__ == "__main__":
    main()